{
    "name": "biases",
    "children": [
   
     {
      "name": "1. Too Much Information",
      "children": [
   
       {
       "name": "We notice things already primed in memory or repeated often.",
        "children": [
{"name": "Availability heuristic", "link": "https:\/\/en.wikipedia.org\/wiki\/Availability_heuristic", "ptags": ["The availability heuristic, also known as availability bias, is a mental shortcut that relies on immediate examples that come to a given person's mind when evaluating a specific topic, concept, method, or decision. This heuristic, operating on the notion that, if something can be recalled, it must be important, or at least more important than alternative solutions not as readily recalled,[1] is inherently biased toward recently acquired information.[2][3]\n", "The mental availability of an action's consequences is positively related to those consequences' perceived magnitude. In other words, the easier it is to recall the consequences of something, the greater those consequences are often perceived to be. Most notably, people often rely on the content of their recall if its implications are not called into question by the difficulty they have in recalling it.[4]\n"]},
{"name": "Attentional bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Attentional_bias", "ptags": ["Attentional bias refers to how a person's perception is affected by selective factors in their attention.[1] Attentional biases may explain an individual's failure to consider alternative possibilities when occupied with an existing train of thought.[2] For example, cigarette smokers have been shown to possess an attentional bias for smoking-related cues around them, due to their brain's altered reward sensitivity.[3] Attentional bias has also been associated with clinically relevant symptoms such as anxiety and depression.[4]\n"]},
{"name": "Illusory truth effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Illusory_truth_effect", "ptags": ["\n", "The illusory truth effect (also known as the illusion of truth effect, validity effect, truth effect, or the reiteration effect) is the tendency to believe false information to be correct after repeated exposure.[1] This phenomenon was first identified in a 1977 study at Villanova University and Temple University.[2][3] When truth is assessed, people rely on whether the information is in line with their understanding or if it feels familiar. The first condition is logical, as people compare new information with what they already know to be true. Repetition makes statements easier to process relative to new, unrepeated statements, leading people to believe that the repeated conclusion is more truthful. The illusory truth effect has also been linked to hindsight bias, in which the recollection of confidence is skewed after the truth has been received.\n", "In a 2012 study, researchers discovered that familiarity can overpower rationality and that repetitively hearing that a certain statement is wrong can paradoxically cause it to feel right.[4] Researchers observed the illusory truth effect's impact even on participants who knew the correct answer to begin with but were persuaded to believe otherwise through the repetition of a falsehood, to \"processing fluency\".\n", "The illusory truth effect plays a significant role in fields such as advertising, news media, and political propaganda.\n"]},
{"name": "Mere exposure effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Mere-exposure_effect", "ptags": ["The mere-exposure effect is a psychological phenomenon by which people tend to develop liking or disliking for things merely because they are familiar with them. In social psychology, this effect is sometimes called the familiarity principle. The effect has been demonstrated with many kinds of things, including words, Chinese characters, paintings, pictures of faces, geometric figures, and sounds.[1] In studies of interpersonal attraction, the more often people see a person, the more pleasing and likeable they find that person.\n"]},
{"name": "Context effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Context_effect", "ptags": ["\n", "A context effect is an aspect of cognitive psychology that describes the influence of environmental factors on one's perception of a stimulus.[1]  The impact of context effects is considered to be part of top-down design.  The concept is supported by the theoretical approach to perception known as constructive perception.  Context effects can impact our daily lives in many ways such as word recognition, learning abilities, memory, and object recognition.  It can have an extensive effect on marketing and consumer decisions.  For example, research has shown that the comfort level of the floor that shoppers are standing on while reviewing products can affect their assessments of product's quality, leading to higher assessments if the floor is comfortable and lower ratings if it is uncomfortable. Because of effects such as this, context effects are currently studied predominantly in marketing.[2]\n"]},
{"name": "Cue-dependent forgetting", "link": "https:\/\/en.wikipedia.org\/wiki\/Cue-dependent_forgetting", "ptags": ["Cue-dependent forgetting, or retrieval failure, is the failure to recall information without memory cues.[1] The term either pertains to semantic cues, state-dependent cues or context-dependent cues.\n", "Upon performing a search for files in a computer, its memory is scanned for words. Relevant files containing this word or string of words are displayed. This is not how memory in the human mind works. Instead, information stored in the memory is retrieved by way of association with other memories. Some memories can not be recalled by simply thinking about them. Rather, one must think about something associated with it.[citation needed]\n", "For example, if someone tries and fails to recollect the memories he had about a vacation he went on, and someone mentions the fact that he hired a classic car during this vacation, this may make him remember all sorts of things from that trip, such as what he ate there, where he went and what books he read.\n"]},
{"name": "Mood-congruent memory bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Mood_congruence", "ptags": ["Mood congruence is the consistency between a person's emotional state with the broader situations and circumstances being experienced by the persons at that time. By contrast, mood incongruence occurs when the individual's reactions or emotional state appear to be in conflict with the situation. In the context of psychosis, hallucinations and delusions may be considered mood congruent (such as feelings of personal inadequacy, guilt, or worthlessness during a bipolar disorder depressive episode) or incongruent.[medical citation needed]\n"]},
{"name": "Frequency illusion", "link": "https:\/\/en.wikipedia.org\/wiki\/Frequency_illusion", "ptags": ["The frequency illusion (also known as the Baader\u2013Meinhof phenomenon) is a cognitive bias in which a person notices a specific concept, word, or product more frequently after recently becoming aware of it.\n", "The name \"Baader\u2013Meinhof phenomenon\" was coined in 1994 by Terry Mullen in a letter to the St. Paul Pioneer Press.[1] The letter describes how, after mentioning the name of the German terrorist group Baader\u2013Meinhof once, he kept noticing it. This led to other readers sharing their own experiences of the phenomenon, leading it to gain recognition. It was not until 2005, when Stanford linguistics professor Arnold Zwicky wrote about this effect on his blog, that the name \"frequency illusion\" was coined.[2]\n"]},
{"name": "Baader-Meinhof Phenomenon", "link": "https:\/\/en.wikipedia.org\/wiki\/Frequency_illusion", "ptags": ["The frequency illusion (also known as the Baader\u2013Meinhof phenomenon) is a cognitive bias in which a person notices a specific concept, word, or product more frequently after recently becoming aware of it.\n", "The name \"Baader\u2013Meinhof phenomenon\" was coined in 1994 by Terry Mullen in a letter to the St. Paul Pioneer Press.[1] The letter describes how, after mentioning the name of the German terrorist group Baader\u2013Meinhof once, he kept noticing it. This led to other readers sharing their own experiences of the phenomenon, leading it to gain recognition. It was not until 2005, when Stanford linguistics professor Arnold Zwicky wrote about this effect on his blog, that the name \"frequency illusion\" was coined.[2]\n"]},
{"name": "Empathy gap", "link": "https:\/\/en.wikipedia.org\/wiki\/Empathy_gap", "ptags": ["An empathy gap, sometimes referred to as an empathy bias, is a breakdown or reduction in empathy (the ability to recognize, understand, and share another's thoughts and feelings) where it might otherwise be expected to occur. Empathy gaps may occur due to a failure in the process of empathizing[1] or as a consequence of stable personality characteristics,[2][3][4] and may reflect either a lack of ability or motivation to empathize.\n", "Empathy gaps can be interpersonal (toward others) or intrapersonal (toward the self, e.g. when predicting one's own future preferences). A great deal of social psychological research has focused on intergroup empathy gaps, their underlying psychological and neural mechanisms, and their implications for downstream behavior (e.g. prejudice toward outgroup members).\n"]},
{"name": "Omission bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Omission_bias", "ptags": ["Omission bias is the phenomenon in which people prefer omission (inaction) over commission (action) and people tend to judge harm as a result of commission more negatively than harm as a result of omission.[1][2][3] It can occur due to a number of processes, including psychological inertia,[4] the perception of transaction costs, and the perception that commissions are more causal than omissions.[5] In social political terms the Universal Declaration of Human Rights establishes how basic human rights are to be assessed in article 2, as \"without distinction of any kind, such as race, colour, sex, language, religion, political or other opinion, national or social origin, property, birth or other status.\" criteria that are often subject to one or another form of omission bias.   It is controversial as to whether omission bias is a cognitive bias or is often rational.[4][6] The bias is often showcased through the trolley problem and has also been described as an explanation for the endowment effect and status quo bias.[2][7]\n"]},
{"name": "Base rate fallacy", "link": "https:\/\/en.wikipedia.org\/wiki\/Base_rate_fallacy", "ptags": ["The base rate fallacy, also called base rate neglect[2] or base rate bias, is a type of fallacy in which people tend to ignore the base rate (e.g., general prevalence) of an occurrence across a population and instead focus upon of the information just relating to that specific case.[3] For example, if someone hears that a friend is very shy and quiet, they might think the friend is more likely to be a librarian than a salesperson, even though there are far more salespeople than librarians overall - hence making it more likely that their friend is actually a salesperson. Base rate neglect is a specific form of the more general extension neglect.\n", "It is also called the prosecutor's fallacy or defense attorney's fallacy when applied to the results of statistical tests (such as DNA tests) in the context of law proceedings. These terms were introduced by William C. Thompson and Edward Schumann in 1987,[4][5] although it has been argued that their definition of the prosecutor's fallacy extends to many additional invalid imputations of guilt or liability that are not analyzable as errors in base rates or Bayes's theorem.[6]\n"]}
        ]
       },
   
       {
         "name": "Bizarre/funny/visually-striking/anthropomorphic things stick out more than non-bizarre/unfunny things.",
         "children": [
{"name": "Bizarreness effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Bizarreness_effect", "ptags": ["Bizarreness effect is the tendency of bizarre material to be better remembered than common material.[1] The scientific evidence for its existence is contested. Some research suggests it does exist, some suggests it doesn't exist and some suggests it leads to worse remembering.[2]\n"]},
{"name": "Humor effect", "link": "https:\/\/en.wikipedia.org\/wiki\/List_of_cognitive_biases#Humor_effect", "ptags": ["Cognitive biases are systematic patterns of deviation from norm and\/or rationality in judgment. They are often studied in psychology, sociology and behavioral economics.[1]\n", "Although the reality of most of these biases is confirmed by reproducible research,[2][3] there are often controversies about how to classify these biases or how to explain them.[4] Several theoretical causes are known for some cognitive biases, which provides a classification of biases by their common generative mechanism (such as noisy information-processing[5]). Gerd Gigerenzer has criticized the framing of cognitive biases as errors in judgment, and favors interpreting them as arising from rational deviations from logical thought.[6]\n", "Explanations include information-processing rules (i.e., mental shortcuts), called heuristics, that the brain uses to produce decisions or judgments. Biases have a variety of forms and appear as cognitive (\"cold\") bias, such as mental noise,[5] or motivational (\"hot\") bias, such as when beliefs are distorted by wishful thinking. Both effects can be present at the same time.[7][8]\n", "There are also controversies over some of these biases as to whether they count as useless or irrational, or whether they result in useful attitudes or behavior. For example, when getting to know others, people tend to ask leading questions which seem biased towards confirming their assumptions about the person. However, this kind of confirmation bias has also been argued to be an example of social skill; a way to establish a connection with the other person.[9]\n", "Although this research overwhelmingly involves human subjects, some findings that demonstrate bias have been found in non-human animals as well. For example, loss aversion has been shown in monkeys and hyperbolic discounting has been observed in rats, pigeons, and monkeys.[10]\n"]},
{"name": "Von Restorff effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Von_Restorff_effect", "ptags": ["The Von Restorff effect, also known as the \"isolation effect\", predicts that when multiple homogeneous stimuli are presented, the stimulus that differs from the rest is more likely to be remembered.[1] The theory was coined by German psychiatrist and pediatrician Hedwig von Restorff (1906\u20131962), who, in her 1933 study, found that when participants were presented with a list of categorically similar items with one distinctive, isolated item on the list, memory for the item was improved.[2]\n", "The study utilized the isolation paradigm, which refers to a distinctive feature of an item in a list that differs from the others by way of dimension. Such distinctiveness, leading to the von Restorff effect, can be generated from changing the meaningfulness or physical nature of the stimulus in some way, such as in size, shape, color, spacing and underlining.\n"]},
{"name": "Picture superiority effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Picture_superiority_effect", "ptags": ["The picture superiority effect refers to the phenomenon in which pictures and images are more likely to be remembered than are words.[2][3][4][5][6][7] This effect has been demonstrated in numerous experiments using different methods. It is based on the notion that \"human memory is extremely sensitive to the symbolic modality of presentation of event information\".[8] Explanations for the picture superiority effect are not concrete and are still being debated, however an evolutionary explanation is that sight has a long history stretching back millions of years and was crucial to survival in the past, whereas reading is a relatively recent invention, and requires specific cognitive processes, such as decoding symbols and linking them to meaning.\n"]},
{"name": "Self-relevance effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Self-reference_effect", "ptags": ["The self-reference effect is a tendency for people to encode information differently depending on whether they are implicated in the information. When people are asked to remember information when it is related in some way to themselves, the recall rate can be improved.[1]\n"]},
{"name": "Negativity bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Negativity_bias", "ptags": ["The negativity bias,[1] also known as the negativity effect, is a cognitive bias that, even when positive or neutral things of equal intensity occur, things of a more negative nature (e.g. unpleasant thoughts, emotions, or social interactions; harmful\/traumatic events) have a greater effect on one's psychological state and processes than neutral or positive things.[2][3][4]  In other words, something very positive will generally have less of an impact on a person's behavior and cognition than something equally emotional but negative.  The negativity bias has been investigated within many different domains, including the formation of impressions and general evaluations; attention, learning, and memory; and decision-making and risk considerations.\n"]}
         ]
       },
   
       {
         "name": "We notice when something has changed.",
         "children": [
   
{"name": "Anchoring", "link": "https:\/\/en.wikipedia.org\/wiki\/Anchoring_(cognitive_bias)", "ptags": ["The anchoring effect is a psychological phenomenon in which an individual's judgements or decisions are influenced by a reference point or \"anchor\" which can be completely irrelevant. Both numeric and non-numeric anchoring have been reported in research. In numeric anchoring, once the value of the anchor is set, subsequent arguments, estimates, etc. made by an individual may change from what they would have otherwise been without the anchor. For example, an individual may be more likely to purchase a car if it is placed alongside a more expensive model (the anchor). Prices discussed in negotiations that are lower than the anchor may seem reasonable, perhaps even cheap to the buyer, even if said prices are still relatively higher than the actual market value of the car.[1] Another example may be when estimating the orbit of Mars, one might start with the Earth's orbit (365 days) and then adjust upward until they reach a value that seems reasonable (usually less than 687 days, the correct answer).\n", "The original description of the anchoring effect came from psychophysics. When judging stimuli along a continuum, it was noticed that the first and last stimuli were used to compare the other stimuli (this is also referred to as \"end anchoring\"). This was applied to attitudes by Sherif et al. in their 1958 article \"Assimilation and effects of anchoring stimuli on judgments\".[2]\n"]},
{"name": "Conservatism", "link": "https:\/\/en.wikipedia.org\/wiki\/Conservatism_(belief_revision)", "ptags": ["In cognitive psychology and decision science, conservatism or conservatism bias is a bias  which refers to the tendency to revise one's belief insufficiently when presented with new evidence. This bias describes human belief revision in which people over-weigh the prior distribution (base rate) and under-weigh new sample evidence when compared to Bayesian belief-revision.\n", "According to the theory, \"opinion change is very orderly, and usually proportional to the numbers of Bayes' theorem \u2013 but it is insufficient in amount\".[1] In other words, people update their prior beliefs as new evidence becomes available, but they do so more slowly than they would if they used Bayes' theorem.\n", "This bias was discussed by Ward Edwards in 1968,[1] who reported on experiments like the following one:\n", "Most subjects chose an answer around .7. The correct answer according to Bayes' theorem is closer to .97 ( based on Bayes' theorem:\n\n\n\n\n\n\n\n0.7\n\n8\n\n\n\u00d7\n\n0.3\n\n4\n\n\n\n\n\n0.7\n\n8\n\n\n\u00d7\n\n0.3\n\n4\n\n\n+\n\n0.3\n\n8\n\n\n\u00d7\n\n0.7\n\n4\n\n\n\n\n\n\n\n{\\displaystyle {\\frac {0.7^{8}\\times 0.3^{4}}{0.7^{8}\\times 0.3^{4}+0.3^{8}\\times 0.7^{4}}}}\n\n). Edwards suggested that people updated beliefs conservatively, in accordance with Bayes' theorem, but more slowly. They updated from .5 incorrectly according to an observed bias in several experiments.[1]\n"]},
{"name": "Contrast effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Contrast_effect", "ptags": ["A contrast effect is the enhancement or diminishment, relative to normal, of perception, cognition or related performance as a result of successive (immediately previous) or simultaneous exposure to a stimulus of lesser or greater value in the same dimension. (Here, normal perception, cognition or performance is that which would be obtained in the absence of the comparison stimulus\u2014i.e., one based on all previous experience.)\n", "Perception example: A neutral gray target will appear lighter or darker than it does in isolation when immediately preceded by, or simultaneously compared to, respectively, a dark gray or light gray target.\n", "Cognition example: A person will appear more or less attractive than that person does in isolation when immediately preceded by, or simultaneously compared to, respectively, a less or more attractive person.\n", "Performance example: A laboratory rat will work faster, or slower, during a stimulus predicting a given amount of reward when that stimulus and reward are immediately preceded by, or alternated with, respectively, different stimuli associated with either a lesser or greater amount of reward.\n"]},
{"name": "Distinction bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Distinction_bias", "ptags": ["Distinction bias, a concept of decision theory, is the tendency to view two options as more distinctive when evaluating them simultaneously than when evaluating them separately.\n", "One writer has presented what he called \"a simplistic view\" of distinction bias: When asked if someone would like an apple, they may say \"Yes\". So, an apple is placed before them and they begin to eat it and are happy. But what if two apples were placed on the table - one was the one they would have happily eaten and the other which is slightly fresher looking. The individual will choose the fresher apple and eat it and be happy but if asked, \"would you have enjoyed eating that other apple\", they would likely say \"No\". Even though in the alternate, no-choice reality they were perfectly happy with the apple. Moreover, if presented with five apples on a table, they might examine each apple so that they would be sure they had the best one, even though the time spent making that decision would be wasted. The reason for this is that distinction bias causes individuals to \"over-examine and over-value the differences between things as we scrutinize them.\"[1]\n"]},
{"name": "Anchoring", "link": "https:\/\/en.wikipedia.org\/wiki\/Anchoring_(cognitive_bias)", "ptags": ["The anchoring effect is a psychological phenomenon in which an individual's judgements or decisions are influenced by a reference point or \"anchor\" which can be completely irrelevant. Both numeric and non-numeric anchoring have been reported in research. In numeric anchoring, once the value of the anchor is set, subsequent arguments, estimates, etc. made by an individual may change from what they would have otherwise been without the anchor. For example, an individual may be more likely to purchase a car if it is placed alongside a more expensive model (the anchor). Prices discussed in negotiations that are lower than the anchor may seem reasonable, perhaps even cheap to the buyer, even if said prices are still relatively higher than the actual market value of the car.[1] Another example may be when estimating the orbit of Mars, one might start with the Earth's orbit (365 days) and then adjust upward until they reach a value that seems reasonable (usually less than 687 days, the correct answer).\n", "The original description of the anchoring effect came from psychophysics. When judging stimuli along a continuum, it was noticed that the first and last stimuli were used to compare the other stimuli (this is also referred to as \"end anchoring\"). This was applied to attitudes by Sherif et al. in their 1958 article \"Assimilation and effects of anchoring stimuli on judgments\".[2]\n"]},
{"name": "Framing effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Framing_effect_(psychology)", "ptags": ["The framing effect is a cognitive bias in which people decide between options based on whether the options are presented with positive or negative connotations.[1] Individuals have a tendency to make risk-avoidant choices when options are positively framed, while selecting more loss-avoidant options when presented with a negative frame. In studies of the bias, options are presented in terms of the probability of either losses or gains. While differently expressed, the options described are in effect identical. Gain and loss are defined in the scenario as descriptions of outcomes, for example, lives lost or saved, patients treated or not treated, monetary gains or losses.[2]\n", "Prospect theory posits that a loss is more significant than the equivalent gain,[2] that a sure gain (certainty effect and pseudocertainty effect) is favored over a probabilistic gain,[3] and that a probabilistic loss is preferred to a definite loss.[2] One of the dangers of framing effects is that people are often provided with options within the context of only one of the two frames.[4]\n", "The concept helps to develop an understanding of frame analysis within social movements, and also in the formation of political opinion where spin plays a large role in political opinion polls that are framed to encourage a response beneficial to the organization that has commissioned the poll. It has been suggested that the use of the technique is discrediting political polls themselves.[5] The effect is reduced, or even eliminated, if ample credible information is provided to people.[5]\n"]},
{"name": "Money illusion", "link": "https:\/\/en.wikipedia.org\/wiki\/Money_illusion", "ptags": ["In economics, money illusion, or price illusion, is a cognitive bias where money is thought of in nominal, rather than real terms. In other words, the face value (nominal value) of money is mistaken for its purchasing power (real value) at a previous point in time. Viewing purchasing power as measured by the nominal value is false, as modern fiat currencies have no intrinsic value and their real value depends purely on the price level. The term was coined by Irving Fisher in Stabilizing the Dollar. It was popularized by John Maynard Keynes in the early twentieth century, and Irving Fisher wrote an important book on the subject, The Money Illusion, in 1928.[1]\n", "The existence of money illusion is disputed by monetary economists  who contend that people act rationally (i.e. think in real prices) with regard to their wealth.[2] Eldar Shafir, Peter A. Diamond, and Amos Tversky (1997) have provided  empirical evidence for the existence of the effect and it has been shown to affect behaviour in a variety of experimental and real-world situations.[3]\n", "Shafir et al.[3] also state that money illusion influences economic behaviour in three main ways:\n", "Money illusion can also influence people's perceptions of outcomes. Experiments have shown that people generally perceive an approximate 2% cut in nominal income with no change in monetary value as unfair, but see a 2% rise in nominal income where there is 4% inflation as fair, despite them being almost rational equivalents. This result is consistent with the 'Myopic Loss Aversion theory'.[4] Furthermore, the money illusion means nominal changes in price can influence demand even if real prices have remained constant.[5]\n"]},
{"name": "Weber\u2013Fechner law", "link": "https:\/\/en.wikipedia.org\/wiki\/Weber\u2013Fechner_law", "ptags": ["The Weber\u2013Fechner laws are two related scientific laws in the field of psychophysics, known as Weber's law and Fechner's law. Both relate to human perception, more specifically the relation between the actual change in a physical stimulus and the perceived change. This includes stimuli to all senses: vision, hearing, taste, touch, and smell.\n", "Ernst Heinrich Weber states that \"the minimum increase of stimulus which will produce a perceptible increase of sensation is proportional to the pre-existent stimulus,\" while Gustav Fechner's law is an inference from Weber's law (with additional assumptions) which states that the intensity of our sensation increases as the logarithm of an increase in energy rather than as rapidly as the increase.[1]\n"]}
   
         ]
       },
   
       {
         "name": "We are drawn to details that confirm our own existing beliefs",
         "children": [
   
{"name": "Confirmation bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Confirmation_bias", "ptags": ["\nConfirmation bias (also confirmatory bias, myside bias,[a] or congeniality bias[2]) is the tendency to search for, interpret, favor, and recall information in a way that confirms or supports one's prior beliefs or values.[3] People display this bias when they select information that supports their views, ignoring contrary information, or when they interpret ambiguous evidence as supporting their existing attitudes. The effect is strongest for desired outcomes, for emotionally charged issues, and for deeply entrenched beliefs. Confirmation bias is insuperable for most people, but they can manage it, for example, by education and training in critical thinking skills.\n", "Biased search for information, biased interpretation of this information, and biased memory recall, have been invoked to explain four specific effects:\n", "A series of psychological experiments in the 1960s suggested that people are biased toward confirming their existing beliefs. Later work re-interpreted these results as a tendency to test ideas in a one-sided way, focusing on one possibility and ignoring alternatives. Explanations for the observed biases include wishful thinking and the limited human capacity to process information. Another proposal is that people show confirmation bias because they are pragmatically assessing the costs of being wrong, rather than investigating in a neutral, scientific way.\n", "Flawed decisions due to confirmation bias have been found in a wide range of political, organizational, financial and scientific contexts. These biases contribute to overconfidence in personal beliefs and can maintain or strengthen beliefs in the face of contrary evidence. For example, confirmation bias produces systematic errors in scientific research based on inductive reasoning (the gradual accumulation of supportive evidence). Similarly, a police detective may identify a suspect early in an investigation, but then may only seek confirming rather than disconfirming evidence. A medical practitioner may prematurely focus on a particular disorder early in a diagnostic session, and then seek only confirming evidence. In social media, confirmation bias is amplified by the use of filter bubbles, or \"algorithmic editing\", which display to individuals only information they are likely to agree with, while excluding opposing views.\n"]},
{"name": "Congruence bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Congruence_bias", "ptags": ["Congruence bias is the tendency of people to over-rely on testing their initial hypothesis (the most congruent one) while neglecting to test alternative hypotheses. That is, people rarely try experiments that could disprove their initial belief, but rather try to repeat their initial results. It is a special case of the confirmation bias.\n"]},
{"name": "Choice-supportive bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Choice-supportive_bias", "ptags": ["Choice-supportive bias or post-purchase rationalization is the tendency to retroactively ascribe positive attributes to an option one has selected and\/or to demote the forgone options.[1] It is part of cognitive science, and is a distinct cognitive bias that occurs once a decision is made. For example, if a person chooses option A instead of option B, they are likely to ignore or downplay the faults of option A while amplifying or ascribing new negative faults to option B. Conversely, they are also likely to notice and amplify the advantages of option A and not notice or de-emphasize those of option B.\n", "What is remembered about a decision can be as important as the decision itself, especially in determining how much regret or satisfaction one experiences.[2] Research indicates that the process of making and remembering choices yields memories that tend to be distorted in predictable ways.[2]\n", "In cognitive science, one predictable way that memories of choice options are distorted is that positive aspects tend to be remembered as part of the chosen option, whether or not they originally were part of that option, and negative aspects tend to be remembered as part of rejected options.[2] Once an action has been taken, the ways in which we evaluate the effectiveness of what we did may be biased.[3] It is believed this may influence our future decision-making. These biases may be stored as memories, which are attributions that we make about our mental experiences based on their subjective qualities, our prior knowledge and beliefs, our motives and goals, and the social context. True and false memories arise by the same mechanism because when the brain processes and stores information, it cannot tell the difference where they came from.[4]\n"]},
{"name": "Choice-supportive bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Choice-supportive_bias", "ptags": ["Choice-supportive bias or post-purchase rationalization is the tendency to retroactively ascribe positive attributes to an option one has selected and\/or to demote the forgone options.[1] It is part of cognitive science, and is a distinct cognitive bias that occurs once a decision is made. For example, if a person chooses option A instead of option B, they are likely to ignore or downplay the faults of option A while amplifying or ascribing new negative faults to option B. Conversely, they are also likely to notice and amplify the advantages of option A and not notice or de-emphasize those of option B.\n", "What is remembered about a decision can be as important as the decision itself, especially in determining how much regret or satisfaction one experiences.[2] Research indicates that the process of making and remembering choices yields memories that tend to be distorted in predictable ways.[2]\n", "In cognitive science, one predictable way that memories of choice options are distorted is that positive aspects tend to be remembered as part of the chosen option, whether or not they originally were part of that option, and negative aspects tend to be remembered as part of rejected options.[2] Once an action has been taken, the ways in which we evaluate the effectiveness of what we did may be biased.[3] It is believed this may influence our future decision-making. These biases may be stored as memories, which are attributions that we make about our mental experiences based on their subjective qualities, our prior knowledge and beliefs, our motives and goals, and the social context. True and false memories arise by the same mechanism because when the brain processes and stores information, it cannot tell the difference where they came from.[4]\n"]},
{"name": "Selective perception", "link": "https:\/\/en.wikipedia.org\/wiki\/Selective_perception", "ptags": ["Selective perception is the tendency not to notice and more quickly forget stimuli that cause emotional discomfort and contradict prior beliefs. For example, a teacher may have a favorite student because they are biased by in-group favoritism. The teacher ignores the student's poor attainment. Conversely, they might not notice the progress of their least favorite student.[1] It can also occur when consuming mass media, allowing people to see facts and opinions they like while ignoring that do not fit with particular opinions, values, beliefs, or frame of reference. Psychologists believe this process occurs automatically.[2]\n"]},
{"name": "Observer-expectancy effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Observer-expectancy_effect", "ptags": ["The observer-expectancy effect (also called the experimenter-expectancy effect, expectancy bias, observer effect, or experimenter effect) is a form of reactivity in which a researcher's cognitive bias causes them to subconsciously influence the participants of an experiment. Confirmation bias can lead to the experimenter interpreting results incorrectly because of the tendency to look for information that conforms to their hypothesis, and overlook information that argues against it.[1] It is a significant threat to a study's internal validity, and is therefore typically controlled using a double-blind experimental design.\n", "It may include conscious or unconscious influences on subject behavior including creation of demand characteristics that influence subjects, and altered or selective recording of experimental results themselves.[2]\n"]},
{"name": "Experimenter's bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Observer-expectancy_effect", "ptags": ["The observer-expectancy effect (also called the experimenter-expectancy effect, expectancy bias, observer effect, or experimenter effect) is a form of reactivity in which a researcher's cognitive bias causes them to subconsciously influence the participants of an experiment. Confirmation bias can lead to the experimenter interpreting results incorrectly because of the tendency to look for information that conforms to their hypothesis, and overlook information that argues against it.[1] It is a significant threat to a study's internal validity, and is therefore typically controlled using a double-blind experimental design.\n", "It may include conscious or unconscious influences on subject behavior including creation of demand characteristics that influence subjects, and altered or selective recording of experimental results themselves.[2]\n"]},
{"name": "Observer effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Observer-expectancy_effect", "ptags": ["The observer-expectancy effect (also called the experimenter-expectancy effect, expectancy bias, observer effect, or experimenter effect) is a form of reactivity in which a researcher's cognitive bias causes them to subconsciously influence the participants of an experiment. Confirmation bias can lead to the experimenter interpreting results incorrectly because of the tendency to look for information that conforms to their hypothesis, and overlook information that argues against it.[1] It is a significant threat to a study's internal validity, and is therefore typically controlled using a double-blind experimental design.\n", "It may include conscious or unconscious influences on subject behavior including creation of demand characteristics that influence subjects, and altered or selective recording of experimental results themselves.[2]\n"]},
{"name": "Expectation bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Observer-expectancy_effect", "ptags": ["The observer-expectancy effect (also called the experimenter-expectancy effect, expectancy bias, observer effect, or experimenter effect) is a form of reactivity in which a researcher's cognitive bias causes them to subconsciously influence the participants of an experiment. Confirmation bias can lead to the experimenter interpreting results incorrectly because of the tendency to look for information that conforms to their hypothesis, and overlook information that argues against it.[1] It is a significant threat to a study's internal validity, and is therefore typically controlled using a double-blind experimental design.\n", "It may include conscious or unconscious influences on subject behavior including creation of demand characteristics that influence subjects, and altered or selective recording of experimental results themselves.[2]\n"]},
{"name": "Ostrich effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Ostrich_effect", "ptags": ["The ostrich effect, also known as the ostrich problem,[1] was originally coined by Galai & Sade (2003).[2] The name comes from the common (but false) legend that ostriches bury their heads in the sand to avoid danger. This effect is a cognitive bias where people tend to \u201cbury their head in the sand\u201d and avoid potentially negative but useful information, such as feedback on progress, to avoid psychological discomfort.[1]\n"]},
{"name": "Barnum effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Barnum_effect", "ptags": ["\n", "The Barnum effect, also called the Forer effect or, less commonly, the Barnum\u2013Forer effect, is a common psychological phenomenon whereby individuals give high accuracy ratings to descriptions of their personality that supposedly are tailored specifically to them, yet which are in fact vague and general enough to apply to a wide range of people.[1] This effect can provide a partial explanation for the widespread acceptance of some paranormal beliefs and practices, such as astrology, fortune telling, aura reading, and some types of personality tests.[1]\n", "Psychologist Bertram Forer originally named it the \"fallacy of personal validation\".[2] The term \"Barnum effect\" was coined in 1956 by psychologist Paul Meehl in his essay \"Wanted \u2013 A Good Cookbook\", because he relates the vague personality descriptions used in certain \"pseudo-successful\" psychological tests to those given by showman P. T. Barnum.[3][4]\n"]},
{"name": "Continued influence effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Confirmation_bias#continued_influence_effect", "ptags": ["\nConfirmation bias (also confirmatory bias, myside bias,[a] or congeniality bias[2]) is the tendency to search for, interpret, favor, and recall information in a way that confirms or supports one's prior beliefs or values.[3] People display this bias when they select information that supports their views, ignoring contrary information, or when they interpret ambiguous evidence as supporting their existing attitudes. The effect is strongest for desired outcomes, for emotionally charged issues, and for deeply entrenched beliefs. Confirmation bias is insuperable for most people, but they can manage it, for example, by education and training in critical thinking skills.\n", "Biased search for information, biased interpretation of this information, and biased memory recall, have been invoked to explain four specific effects:\n", "A series of psychological experiments in the 1960s suggested that people are biased toward confirming their existing beliefs. Later work re-interpreted these results as a tendency to test ideas in a one-sided way, focusing on one possibility and ignoring alternatives. Explanations for the observed biases include wishful thinking and the limited human capacity to process information. Another proposal is that people show confirmation bias because they are pragmatically assessing the costs of being wrong, rather than investigating in a neutral, scientific way.\n", "Flawed decisions due to confirmation bias have been found in a wide range of political, organizational, financial and scientific contexts. These biases contribute to overconfidence in personal beliefs and can maintain or strengthen beliefs in the face of contrary evidence. For example, confirmation bias produces systematic errors in scientific research based on inductive reasoning (the gradual accumulation of supportive evidence). Similarly, a police detective may identify a suspect early in an investigation, but then may only seek confirming rather than disconfirming evidence. A medical practitioner may prematurely focus on a particular disorder early in a diagnostic session, and then seek only confirming evidence. In social media, confirmation bias is amplified by the use of filter bubbles, or \"algorithmic editing\", which display to individuals only information they are likely to agree with, while excluding opposing views.\n"]},
{"name": "Semmelweis reflex", "link": "https:\/\/en.wikipedia.org\/wiki\/Semmelweis_reflex", "ptags": ["The Semmelweis reflex or \"Semmelweis effect\" is a metaphor for the reflex-like tendency to reject new evidence or new knowledge because it contradicts established norms, beliefs, or paradigms.[1]\n"]}
   
         ]
       },
   
       {
         "name": "We notice flaws in others more easily than flaws in ourselves.",
         "children": [
   
{"name": "Bias blind spot", "link": "https:\/\/en.wikipedia.org\/wiki\/Bias_blind_spot", "ptags": ["The bias blind spot is the cognitive bias of recognizing the impact of biases on the judgment of others, while failing to see the impact of biases on one's own judgment.[1] The term was created by Emily Pronin, a social psychologist from Princeton University's Department of Psychology, with colleagues Daniel Lin and Lee Ross.[2][better\u00a0source\u00a0needed] The bias blind spot is named after the visual blind spot. Most people appear to exhibit the bias blind spot. In a sample of more than 600 residents of the United States, more than 85% believed they were less biased than the average American. Only one participant believed that they were more biased than the average American. People do vary with regard to the extent to which they exhibit the bias blind spot. This phenomenon has been successfully replicated and it appears that in general, stronger personal free will beliefs are associated with bias blind spot.[3] It appears to be a stable individual difference that is measurable.[5]\n", "The bias blind spot appears to be a true blind spot in that it is unrelated to actual decision making ability. Performance on indices of decision making competence are not related to individual differences in bias blind spot. In other words, most people appear to believe that they are less biased than others, regardless of their actual decision making ability.[4]\n"]},
{"name": "Na\u00efve cynicism", "link": "https:\/\/en.wikipedia.org\/wiki\/Na\u00efve_cynicism", "ptags": ["Na\u00efve cynicism is a philosophy of mind, cognitive bias and form of psychological egoism that occurs when people na\u00efvely expect more egocentric bias in others than actually is the case.\n", "The term was formally proposed by Justin Kruger and Thomas Gilovich and has been studied across a wide range of contexts including: negotiations,[1] group-membership,[2] marriage,[2] economics,[3] government policy[4] and more.\n"]},
{"name": "Na\u00efve realism", "link": "https:\/\/en.wikipedia.org\/wiki\/Na\u00efve_realism_(psychology)", "ptags": ["In social psychology, na\u00efve realism\u00a0is the human tendency to believe that we see the world around us objectively, and that people who disagree with us must be uninformed, irrational, or biased.\n", "Na\u00efve realism provides a theoretical basis for several other cognitive biases, which are systematic errors when it comes to thinking and making decisions. These include the false consensus effect, actor\u2013observer bias, bias blind spot, and fundamental attribution error, among others.\n", "The term, as it is used in psychology today, was coined by social psychologist Lee Ross and his colleagues in the 1990s.[1][2] It is related to the philosophical concept of na\u00efve realism, which is the idea that our senses allow us to perceive objects directly and without any intervening processes.[3] Social psychologists in the mid-20th century argued against this stance and proposed instead that perception is inherently subjective.[4]\n", "Several prominent social psychologists have studied na\u00efve realism experimentally, including\u00a0Lee Ross, Andrew Ward, Dale Griffin, Emily Pronin, Thomas Gilovich, Robert Robinson, and Dacher Keltner. In 2010, the Handbook of Social Psychology recognized na\u00efve realism as one of \"four hard-won insights about human perception, thinking, motivation and behavior that\u00a0... represent important, indeed foundational, contributions of social psychology.\"[5]\n"]}
   
         ]
       }
   
     ]
   },
   
     {
      "name": "2. Not Enough Meaning",
      "children": [
       {
       "name": "We find stories and patterns even in sparse data",
        "children": [
   
{"name": "Confabulation", "link": "https:\/\/en.wikipedia.org\/wiki\/Confabulation", "ptags": ["In psychology, confabulation is a memory error consisting of the production of fabricated, distorted, or misinterpreted memories about oneself or the world. It is generally associated with certain types of brain damage (especially aneurysm in the anterior communicating artery) or a specific subset of dementias.[1] While still an area of ongoing research, the basal forebrain is implicated in the phenomenon of confabulation. People who confabulate present with incorrect memories ranging from subtle inaccuracies to surreal fabrications, and may include confusion or distortion in the temporal framing (timing, sequence or duration) of memories.[2] In general, they are very confident about their recollections, even when challenged with contradictory evidence.[3]\n", "Confabulation occurs when individuals mistakenly recall false information, without intending to deceive. Brain damage, dementia, and anticholinergic toxidrome can cause this distortion. Two types of confabulation exist: provoked and spontaneous, with two distinctions: verbal and behavioral. Verbal statements, false information, and the patient's unawareness of the distortion are all associated with this phenomenon. Personality structure also plays a role in confabulation.\n", "Numerous theories have been developed to explain confabulation. Neuro\u00adpsycho\u00adlog\u00adi\u00adcal theories suggest that cognitive dysfunction causes the distortion. Self-identity theories posit that people confabulate to preserve themselves. The temporality theory believes that confabulation occurs when an individual cannot place events properly in time. The monitoring and strategic retrieval account theories argue that confabulation arises when individuals cannot recall memories correctly or monitor them after retrieval. The executive control and fuzzy-trace theories also attempt to explain why confabulation happens.\n", "Confabulation can occur with nervous system injuries or illnesses, including Korsakoff's syndrome, Alzheimer's disease, schizophrenia, and traumatic brain injury. It is believed that the right frontal lobe of the brain is damaged, causing false memories. Children are especially susceptible to forced confabulation as they are highly impressionable. Feedback can increase confidence in false memories. In rare cases, confabulation occurs in ordinary individuals.\n", "Different memory tests, including recognition tasks and free recall tasks, can be used to study confabulation. Treatment depends on the underlying cause of the distortion. Ongoing research aims to develop a standard test battery to discern between different types of confabulations, distinguish delusions from confabulations, understand the role of unconscious processes, and identify pathological and nonpathological confabulations.\n"]},
{"name": "Clustering illusion", "link": "https:\/\/en.wikipedia.org\/wiki\/Clustering_illusion", "ptags": ["The clustering illusion is the tendency to erroneously consider the inevitable \"streaks\" or \"clusters\" arising in small samples from random distributions to be non-random. The illusion is caused by a human tendency to underpredict the amount of variability likely to appear in a small sample of random or pseudorandom data.[1]\n", "Thomas Gilovich, an early author on the subject, argued that the effect occurs for different types of random dispersions. Some might perceive patterns in stock market price fluctuations over time, or clusters in two-dimensional data such as the locations of impact of World War II V-1 flying bombs on maps of London.[1][2] Although Londoners developed specific theories about the pattern of impacts within London, a statistical analysis by R. D. Clarke originally published in 1946 showed that the impacts of V-2 rockets on London were a close fit to a random distribution.[3][4][5][6][7]\n"]},
{"name": "Insensitivity to sample size", "link": "https:\/\/en.wikipedia.org\/wiki\/Insensitivity_to_sample_size", "ptags": ["Insensitivity to sample size is a cognitive bias that occurs when people judge the probability of obtaining a sample statistic without respect to the sample size. For example, in one study, subjects assigned the same probability to the likelihood of obtaining a mean height of above six feet [183\u00a0cm] in samples of 10, 100, and 1,000 men. In other words, variation is more likely in smaller samples, but people may not expect this.[1]\n", "In another example, Amos Tversky and Daniel Kahneman asked subjects\n", "56% of subjects chose option 3, and 22% of subjects respectively chose options 1 or 2. However, according to sampling theory the larger hospital is much more likely to report a sex ratio close to 50% on a given day than the smaller hospital which requires that the correct answer to the question is the smaller hospital (see the law of large numbers).\n", "Relative neglect of sample size were obtained in a different study of statistically sophisticated psychologists.[2]\n", "Tversky and Kahneman explained these results as being caused by the representativeness heuristic, according to which people intuitively judge samples as having similar properties to their population without taking other considerations into effect. A related bias is the clustering illusion, in which people under-expect streaks or runs in small samples. Insensitivity to sample size is a subtype of extension neglect.[3]\n", "To illustrate this point, Howard Wainer and Harris L. Zwerling demonstrated that kidney cancer rates are lowest in counties that are mostly rural, sparsely populated, and located in traditionally Republican states in the Midwest, the South, and the West, but that they are also highest in counties that are mostly rural, sparsely populated, and located in traditionally Republican states in the Midwest, the South, and the West. While various environmental and economic reasons could be advanced for these facts, Wainer and Zwerlig argue that this is an artifact of sample size.  Because of the small sample size, the incidence of a certain kind of cancer in small rural counties is more likely to be further from the mean, in one direction or another, than the incidence of the same kind of cancer in much more heavily populated urban counties.[4]\n"]},
{"name": "Neglect of probability", "link": "https:\/\/en.wikipedia.org\/wiki\/Neglect_of_probability", "ptags": ["The neglect of probability, a type of cognitive bias, is the tendency to disregard probability when making a decision under uncertainty and is one simple way in which people regularly violate the normative rules for decision making. Small risks are typically either neglected entirely or hugely overrated. The continuum between the extremes is ignored. The term probability neglect was coined by Cass Sunstein.[1]\n", "There are many related ways in which people violate the normative rules of decision making with regard to probability including the hindsight bias, the neglect of prior base rates effect, and the gambler's fallacy. However, this bias is different, in that, rather than incorrectly using probability, the actor disregards it.\n", "\"We have no intuitive grasp of risk and thus distinguish poorly among different threats,\" Dobelli has written. \"The more serious the threat and the more emotional the topic (such as radioactivity), the less reassuring a reduction in risk seems to us.\"[2]\n"]},
{"name": "Anecdotal fallacy", "link": "https:\/\/en.wikipedia.org\/wiki\/Anecdotal_evidence", "ptags": ["Anecdotal evidence is evidence based only on personal observation, collected in a casual or non-systematic manner. \n", "When used in advertising or promotion of a product, service, or idea, anecdotal reports are often called a testimonial, which are highly regulated[1] in some jurisdictions.\n", "When compared to other types of evidence, anecdotal evidence is generally regarded as limited in value due to a number of potential weaknesses, but may be considered within the scope of scientific method as some anecdotal evidence can be both empirical and verifiable, e.g. in the use of case studies in medicine. Other anecdotal evidence, however, does not qualify as scientific evidence, because its nature prevents it from being investigated by the scientific method. Where only one or a few anecdotes are presented, there is a larger chance that they may be unreliable due to cherry-picked or otherwise non-representative samples of typical cases.[2][3] Similarly, psychologists have found that due to cognitive bias people are more likely to remember notable or unusual examples rather than typical examples.[4] Thus, even when accurate, anecdotal evidence is not necessarily representative of a typical experience. Accurate determination of whether an anecdote is typical requires statistical evidence.[5] Misuse of anecdotal evidence in the form of argument from anecdote is an informal fallacy[6] and is sometimes referred to as the \"person who\" fallacy (\"I know a person who...\"; \"I know of a case where...\" etc.) which places undue weight on experiences of close peers which may not be typical.\n"]},
{"name": "Illusion of validity", "link": "https:\/\/en.wikipedia.org\/wiki\/Illusion_of_validity", "ptags": ["Illusion of validity is a cognitive bias in which a person overestimates their ability to interpret and predict accurately the outcome when analyzing a set of data, in particular when the data analyzed show a very consistent pattern\u2014that is, when the data \"tell\" a coherent story.[1][2]\n", "This effect persists even when the person is aware of all the factors that limit the accuracy of their predictions, that is when the data and\/or methods used to judge them lead to highly fallible predictions.[2]\n", "Daniel Kahneman, Paul Slovic, and Amos Tversky explain the illusion as follows: \"people often predict by selecting the output...that is most representative of the input....The confidence they have in their prediction depends primarily on the degree of representativeness...with little or no regard for the factors that limit predictive accuracy. Thus, people express great confidence in the prediction that a person is a librarian when given a description of his personality which matches the stereotype of librarians, even if the description is scanty, unreliable, or outdated. The unwarranted confidence which is produced by a good fit between the predicted outcome and the input information may be called the illusion of validity.\"[3]\n", "Consistent patterns may be observed when input variables are highly redundant or correlated, which may increase subjective confidence. However, a number of highly correlated inputs should not increase confidence much more than only one of the inputs; instead higher confidence should be merited when a number of highly independent inputs show a consistent pattern.[2]\n"]},
{"name": "Masked man fallacy", "link": "https:\/\/en.wikipedia.org\/wiki\/Masked-man_fallacy", "ptags": ["In philosophical logic, the masked-man fallacy (also known as the intensional fallacy or epistemic fallacy)[1] is committed when one makes an illicit use of Leibniz's law in an argument. Leibniz's law states that if A and B are the same object, then A and B are indiscernible (that is, they have all the same properties). By modus tollens, this means that if one object has a certain property, while another object does not have the same property, the two objects cannot be identical. The fallacy is \"epistemic\" because it posits an immediate identity between a subject's knowledge of an object with the object itself, failing to recognize that Leibniz's Law is not capable of accounting for intensional contexts.\n"]},
{"name": "Recency illusion", "link": "https:\/\/en.wikipedia.org\/wiki\/Recency_illusion", "ptags": ["The recency illusion is the belief or impression, on the part of someone who has only recently become aware of a long-established phenomenon, that the phenomenon itself must be of recent origin. The term was coined by Arnold Zwicky, a linguist at Stanford University primarily interested in examples involving words, meanings, phrases, and grammatical constructions.[1] However, use of the term is not restricted to linguistic phenomena: Zwicky has defined it simply as, \"the belief that things you have noticed only recently are in fact recent\".[2]\n", "According to Zwicky, the illusion is caused by selective attention.[2]\n"]},
{"name": "Gambler's fallacy", "link": "https:\/\/en.wikipedia.org\/wiki\/Gambler's_fallacy", "ptags": ["The gambler's fallacy, also known as the Monte Carlo fallacy or the fallacy of the maturity of chances, is the belief that, if an event (whose occurrences are independent and identically distributed) has occurred more frequently than expected, it is less likely to happen again in the future (or vice versa). The fallacy is commonly associated with gambling, where it may be believed, for example, that the next dice roll is more than usually likely to be six because there have recently been fewer than the expected number of sixes.\n", "The term  \"Monte Carlo fallacy\" originates from an example of the phenomenon, in which the roulette wheel spun black 26 times in succession at the Monte Carlo Casino in 1913.[1]\n"]},
{"name": "Hot-hand fallacy", "link": "https:\/\/en.wikipedia.org\/wiki\/Hot-hand_fallacy", "ptags": ["The \"hot hand\" (also known as the \"hot hand phenomenon\" or \"hot hand fallacy\") is a phenomenon, previously considered a cognitive social bias,[1][circular reference] that a person who experiences a successful outcome has a greater chance of success in further attempts. The concept is often applied to sports and skill-based tasks in general and originates from basketball, where a shooter is more likely to score if their previous attempts were successful; i.e., while having the \"hot hand.\u201d While previous success at a task can indeed change the psychological attitude and subsequent success rate of a player, researchers for many years did not find evidence for a \"hot hand\" in practice, dismissing it as fallacious. However, later research questioned whether the belief is indeed a fallacy.[2][3] Some recent studies using modern statistical analysis have observed evidence for the \"hot hand\" in some sporting activities;[3] however, other recent studies have not observed evidence of the \"hot hand\".[4] Moreover, evidence suggests that only a small subset of players may show a \"hot hand\" and, among those who do, the magnitude (i.e., effect size) of the \"hot hand\" tends to be small.[5]\n"]},
{"name": "Illusory correlation", "link": "https:\/\/en.wikipedia.org\/wiki\/Illusory_correlation", "ptags": ["In psychology, illusory correlation is the phenomenon of perceiving a relationship between variables (typically people, events, or behaviors) even when no such relationship exists. A false association may be formed because rare or novel occurrences are more salient and therefore tend to capture one's attention.[1] This phenomenon is one way stereotypes form and endure.[2][3] Hamilton & Rose (1980) found that stereotypes can lead people to expect certain groups and traits to fit together, and then to overestimate the frequency with which these correlations actually occur.[4] These stereotypes can be learned and perpetuated without any actual contact occurring between the holder of the stereotype and the group it is about.\n"]},
{"name": "Pareidolia", "link": "https:\/\/en.wikipedia.org\/wiki\/Pareidolia", "ptags": ["\n", "Pareidolia (\/\u02ccp\u00e6r\u026a\u02c8do\u028ali\u0259, \u02ccp\u025b\u0259r-\/;[1] also US: \/\u02ccp\u025b\u0259ra\u026a-\/)[2] is the tendency for perception to impose a meaningful interpretation on a nebulous stimulus, usually visual, so that one detects an object, pattern, or meaning where there is none. Pareidolia is a type of apophenia.\n", "Common examples include  perceived images of animals, faces, or objects in cloud formations; seeing faces in inanimate objects; or lunar pareidolia like the Man in the Moon or the Moon rabbit. The concept of pareidolia may extend to include hidden messages in recorded music played in reverse or at higher- or lower-than-normal speeds, and hearing voices (mainly indistinct) or music in random noise, such as that produced by air conditioners or by fans.[3][4]\n"]},
{"name": "Anthropomorphism", "link": "https:\/\/en.wikipedia.org\/wiki\/Anthropomorphism#Psychology", "ptags": ["\n", "Anthropomorphism is the attribution of human traits, emotions, or intentions to non-human entities.[1] It is considered to be an innate tendency of human psychology.[2] Personification is the related attribution of human form and characteristics to abstract concepts such as nations, emotions, and natural forces, such as seasons and weather. Both have ancient roots as storytelling and artistic devices, and most cultures have traditional fables with anthropomorphized animals as characters. People have also routinely attributed human emotions and behavioral traits to wild as well as domesticated animals.[3]\n"]}
   
        ]
       },
   
       {
       "name": "We fill in characteristics from stereotypes, generalities, and prior histories",
        "children": [
   
{"name": "Group attribution error", "link": "https:\/\/en.wikipedia.org\/wiki\/Group_attribution_error", "ptags": ["The group attribution error refers to people's tendency to believe either\n", "The group attribution error shares an attribution bias analogous to the fundamental attribution error.[2] Rather than focusing on individual's behavior, it relies on group outcomes and attitudes as its main basis for conclusions.\n"]},
{"name": "Ultimate attribution error", "link": "https:\/\/en.wikipedia.org\/wiki\/Ultimate_attribution_error", "ptags": ["The ultimate attribution error is a type of attribution error which describes how attributions of outgroup behavior are more negative than ingroup behavior.[1] As a cognitive bias, the error results in negative outgroup behavior being more likely to be attributed to factors internal and specific to the actor, such as personality, and the attribution of negative ingroup behavior to external factors such as luck or circumstance.[1] The bias reinforces negative stereotypes and prejudice about the outgroup and favouritism of the ingroup through positive stereotypes.[2] The theory also extends to the bias that positive acts performed by ingroup members are more likely a result of their personality.[3]\n", "Four categories have been identified that describe the negative attribution of positive outgroup behaviour. First, that the outgroup member is an exception to a general rule; second, that the member was lucky or had specific advantages; third, that the member was highly motivated; and lastly that the behaviour as attributable to situational causes.[2]\n", "The concept and term originates in an article by Thomas F. Pettigrew in 1979 as an extension of the fundamental attribution error which was identified in 1958.[1][4] Since its publication, which at the time lacked a strong empirical basis, there has been some support for the theory.[1] The specific categorisation originally proposed had only some empirical support for broader categories of motivational and cognitive attribution.[1] [5] The bias is related to intergroup attribution bias.[5][6]\n"]},
{"name": "Stereotyping", "link": "https:\/\/en.wikipedia.org\/wiki\/Stereotype", "ptags": ["\n", "In social psychology, a stereotype is a generalized belief about a particular category of people.[2] It is an expectation that people might have about every person of a particular group. The type of expectation can vary; it can be, for example, an expectation about the group's personality, preferences, appearance or ability.  Stereotypes are often overgeneralized, inaccurate, and resistant to new information.[3] A stereotype does not necessarily need to be a negative assumption. They may be positive, neutral, or negative.\n"]},
{"name": "Essentialism", "link": "https:\/\/en.wikipedia.org\/wiki\/Essentialism", "ptags": ["\n", "Essentialism is the view that objects have a set of attributes that are necessary to their identity.[1] In early Western thought, Platonic idealism held that all things have such an \"essence\"\u2014an \"idea\" or \"form\". In Categories, Aristotle similarly proposed that all objects have a substance that, as George Lakoff put it, \"make the thing what it is, and without which it would be not that kind of thing\".[2] The contrary view\u2014non-essentialism\u2014denies the need to posit such an \"essence\". Essentialism has been controversial from its beginning. In the Parmenides dialogue, Plato depicts Socrates questioning the notion, suggesting that if we accept the idea that every beautiful thing or just action partakes of an essence to be beautiful or just, we must also accept the \"existence of separate essences for hair, mud, and dirt\".[3]\n", "Older social theories were often conceptually essentialist.[4] In biology and other natural sciences, essentialism provided the rationale for taxonomy at least until the time of Charles Darwin.[5] The role and importance of essentialism in modern biology is still a matter of debate.[6] Beliefs which posit that social identities such as race, ethnicity, nationality, or gender are essential characteristics have been central to many discriminatory or extremist ideologies.[7] For instance, psychological essentialism is correlated with racial prejudice.[8][9] Essentialist views about race have also been shown to diminish empathy when dealing with members of another racial group.[10] In medical sciences, essentialism can lead to a reified view of identities, leading to fallacious conclusions and potentially unequal treatment.[11]\n"]},
{"name": "Functional fixedness", "link": "https:\/\/en.wikipedia.org\/wiki\/Functional_fixedness", "ptags": ["Functional fixedness is a cognitive bias that limits a person to use an object only in the way it is traditionally used. The concept of functional fixedness originated in Gestalt psychology, a movement in psychology that emphasizes holistic processing. Karl Duncker defined functional fixedness as being a mental block against using an object in a new way that is required to solve a problem.[1] This \"block\" limits the ability of an individual to use components given to them to complete a task, as they cannot move past the original purpose of those components. For example, if someone needs a paperweight, but they only have a hammer, they may not see how the hammer can be used as a paperweight. Functional fixedness is this inability to see a hammer's use as anything other than for pounding nails; the person couldn't think to use the hammer in a way other than in its conventional function.\n", "When tested, 5-year-old children show no signs of functional fixedness. It has been argued that this is because at age 5, any goal to be achieved with an object is equivalent to any other goal. However, by age 7, children have acquired the tendency to treat the originally intended purpose of an object as special.[2]\n"]},
{"name": "Moral credential effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Self-licensing", "ptags": ["Self-licensing (also moral self-licensing, moral licensing, or licensing effect) is a term used in social psychology and marketing to describe the subconscious phenomenon whereby increased confidence and security in one's self-image or self-concept tends to make that individual worry less about the consequences of subsequent immoral behavior and, therefore, more likely to make immoral choices and act immorally.[1][2][3][4][5][6] In simple terms, self-licensing occurs when people allow themselves to indulge after doing something positive first; for example, drinking a diet soda with a greasy hamburger and fries can lead one to subconsciously discount the negative attributes of the meal's high caloric and cholesterol content.[7]\n", "A large subset of this effect, the moral credential effect, is a bias that occurs when a person's track record as a good egalitarian establishes in them an unconscious ethical certification, endorsement, or license that increases the likelihood of less egalitarian decisions later. This effect occurs even when the audience or moral peer group is unaware of the affected person's previously established moral credential. For example, individuals who had the opportunity to recruit a woman or Black person in one setting were more likely to say later, in a different setting, that a job would be better suited for a man or a white person.[5] Similar effects also appear to occur when a person observes another person from a group they identify with making an egalitarian decision.[8]\n", "Self-licensing can have negative societal consequences since it has a permissive effect on behaviors such as racial prejudice and discrimination, selfishness, poor dietary and health habits, and excessive energy consumption.\n", "But recent scholarship has failed to replicate seminal studies of the licensing effect, and meta-analysis found it to be exaggerated by publication bias.[9][10] Furthermore, where licensing typically assumes that a good deed is the cause that makes subsequent transgressions more likely, an alternative (or additional) account is that people are faced with a temptation to do something morally dubious, and use a prior good deed as an excuse or reason why it is allowed for them to indulge.[11]\n"]},
{"name": "Just-world hypothesis", "link": "https:\/\/en.wikipedia.org\/wiki\/Just-world_hypothesis", "ptags": ["The just-world hypothesis, or just-world fallacy, is the cognitive bias that assumes that \"people get what they deserve\" \u2013 that actions will necessarily have morally fair and fitting consequences for the actor. For example, the assumptions that noble actions will eventually be rewarded and evil actions will eventually be punished fall under this hypothesis. In other words, the just-world hypothesis is the tendency to attribute consequences to\u2014or expect consequences as the result of\u2014 either a universal force that restores moral balance or a universal connection between the nature of actions and their results. This belief generally implies the existence of cosmic justice, destiny, divine providence, desert, stability, order, or the anglophone colloquial use of \"Karma\". It is often associated with a variety of fundamental fallacies, especially in regard to rationalizing suffering on the grounds that the sufferers \"deserve\" it.\n", "The hypothesis popularly appears in the English language in various figures of speech that imply guaranteed punishment for wrongdoing, such as: \"you got what was coming to you\", \"what goes around comes around\", \"chickens come home to roost\", \"everything happens for a reason\", and \"you reap what you sow\". This hypothesis has been widely studied by social psychologists since Melvin J. Lerner conducted seminal work on the belief in a just world in the early 1960s.[1] Research has continued since then, examining the predictive capacity of the hypothesis in various situations and across cultures, and clarifying and expanding the theoretical understandings of just-world beliefs.[2]\n"]},
{"name": "Argument from fallacy", "link": "https:\/\/en.wikipedia.org\/wiki\/Argument_from_fallacy", "ptags": ["Argument from fallacy is the formal fallacy of analyzing an argument and inferring that, since it contains a fallacy, its conclusion must be false.[1] It is also called argument to logic  (argumentum ad logicam), the fallacy fallacy,[2] the fallacist's fallacy,[3] and the bad reasons fallacy.[4]\n"]},
{"name": "System justification", "link": "https:\/\/en.wikipedia.org\/wiki\/System_justification", "ptags": ["System justification theory is a theory within social psychology that system-justifying beliefs serve a psychologically palliative function. It proposes that people have several underlying needs, which vary from individual to individual, that can be satisfied by the defense and justification of the status quo, even when the system may be disadvantageous to certain people. People have epistemic, existential, and relational needs that are met by and manifest as ideological support for the prevailing structure of social, economic, and political norms. Need for order and stability, and thus resistance to change or alternatives, for example, can be a motivator for individuals to see the status quo as good, legitimate, and even desirable.\n", "According to system justification theory, people desire not only to hold favorable attitudes about themselves (ego-justification) and the groups to which they belong (group-justification), but also to hold positive attitudes about the overarching social structure in which they are entwined and find themselves obligated to (system-justification). This system-justifying motive sometimes produces the phenomenon known as out-group favoritism, an acceptance of inferiority among low-status groups and a positive image of relatively higher status groups. Thus, the notion that individuals are simultaneously supporters and victims of the system-instilled norms is a central idea in system justification theory. Additionally, the passive ease of supporting the current structure, when compared to the potential price (material, social, psychological) of acting out against the status quo, leads to a shared environment in which the existing social, economic, and political arrangements tend to be preferred. Alternatives to the status quo tend to be disparaged, and inequality tends to perpetuate.[1][2]\n"]},
{"name": "Automation bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Automation_bias", "ptags": ["Automation bias is the propensity for humans to favor suggestions from automated decision-making systems and to ignore contradictory information made without automation, even if it is correct.[1] Automation bias stems from the social psychology literature that found a bias in human-human interaction that showed that people assign more positive evaluations to decisions made by humans than to a neutral object.[2] The same type of positivity bias has been found for human-automation interaction,[3] where the automated decisions are rated more positively than neutral.[4] This has become a growing problem for decision making as intensive care units, nuclear power plants, and aircraft cockpits have increasingly integrated computerized system monitors and decision aids to mostly factor out possible human error. Errors of automation bias tend to occur when decision-making is dependent on computers or other automated aids and the human is in an observatory role but able to make decisions. Examples of automation bias range from urgent matters like flying a plane on automatic pilot to such mundane matters as the use of spell-checking programs.[5]\n"]},
{"name": "Bandwagon effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Bandwagon_effect", "ptags": ["The bandwagon effect is a psychological phenomenon where people  adopt certain behaviors, styles, or attitudes simply because others are doing so.[1] More specifically, it is a cognitive bias by which public opinion or behaviours can alter due to particular actions and beliefs rallying amongst the public.[2] It is a psychological phenomenon whereby the rate of uptake of beliefs, ideas, fads and trends increases with respect to the proportion of others who have already done so.[3] As more people come to believe in something, others also \"hop on the bandwagon\" regardless of the underlying evidence.\n", "Following others' actions or beliefs can occur because of conformism or deriving information from others. Much of the influence of the bandwagon effect comes from the desire to 'fit in' with peers; by making similar selections as other people, this is seen as a way to gain access to a particular social group.[4] An example of this is fashion trends wherein the increasing popularity of a certain garment or style encourages more acceptance.[5] When individuals make rational choices based on the information they receive from others, economists have proposed that information cascades can quickly form in which people ignore their personal information signals and follow the behaviour of others.[6] Cascades explain why behaviour is fragile as people understand that their behaviour is based on a very limited amount of information. As a result, fads form easily but are also easily dislodged.[citation needed] The phenomenon is observed in various fields, such as economics, political science, medicine, and psychology.[7] In social psychology, people's tendency to align their beliefs and behaviors with a group is known as 'herd mentality' or 'groupthink'.[8] The reverse bandwagon effect (also known as the snob effect in certain contexts) is a cognitive bias that causes people to avoid doing something, because they believe that other people are doing it.[9]\n"]},
{"name": "Placebo effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Placebo", "ptags": ["A placebo (\/pl\u0259\u02c8si\u02d0bo\u028a\/ pl\u0259-SEE-boh) can be roughly defined as a sham medical treatment.[1] Common placebos include inert tablets (like sugar pills), inert injections (like saline), sham surgery,[2] and other procedures.[3]\n", "Placebos are used in randomized clinical trials to test the efficacy of medical treatments, so they serve as epistemological tools to screen out the \u2018noise\u2019 of clinical research. Placebos in clinical trials should ideally be indistinguishable from so-called verum treatments under investigation, except for the latter's particular hypothesized remedial factor(s).[4] This is to prevent the recipient or others from knowing (with their consent) whether a treatment is active or inactive, as expectations about efficacy can influence results.[5][6]\n", "Placebos are also popular because they can sometimes produce relief through psychological mechanisms (a phenomenon known as the \"placebo effect\"). They can affect how patients perceive their condition and encourage the body's chemical processes for relieving pain[7] and a few other symptoms,[8] but have no impact on the disease itself.[9][7]\n", "Improvements that patients experience after being treated with a placebo can also be due to unrelated factors, such as regression to the mean (a statistical effect where an unusually high or low measurement is likely to be followed by a less extreme one).[7] The use of placebos in clinical medicine raises ethical concerns, especially if they are disguised as an active treatment, as this introduces dishonesty into the doctor\u2013patient relationship and bypasses informed consent.[10]\n", "In a placebo-controlled clinical trial any change in the control group is known as the placebo response, and the difference between this and the result of no treatment is the placebo effect.[11] Some researchers now recommend comparing the experimental treatment with an existing treatment when possible, instead of a placebo.[12]\n", "The idea of a placebo effect was discussed in 18th century psychology,[13] but became more prominent in the 20th century. Modern studies find that placebos can affect some outcomes such as pain and nausea, but otherwise do not generally have important clinical effects.[9]\n"]}
        ]
       },
   
       {
       "name": "We imagine things and people we're familiar with or fond of as better",
        "children": [
   
{"name": "Out-group homogeneity bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Out-group_homogeneity", "ptags": ["The out-group homogeneity effect is the perception of out-group members as more similar to one another than are in-group members, e.g. \"they are alike; we are diverse\".[1] Perceivers tend to have impressions about the diversity or variability of group members around those central tendencies or typical attributes of those group members. Thus, outgroup stereotypicality judgments are overestimated, supporting the view that out-group stereotypes are overgeneralizations.[2] The term \"outgroup homogeneity effect\", \"outgroup homogeneity bias\" or \"relative outgroup homogeneity\" have been explicitly contrasted with \"outgroup homogeneity\" in general,[3] the latter referring to perceived outgroup variability unrelated to perceptions of the ingroup.\n", "The outgroup homogeneity effect is sometimes referred to as \"outgroup homogeneity bias\". Such nomenclature hints at a broader meta-theoretical debate that is present in the field of social psychology. This debate centres on the validity of heightened perceptions of ingroup and outgroup homogeneity, where some researchers view the homogeneity effect as an example of cognitive bias and error, while other researchers view the effect as an example of normal and often adaptive social perception.[3] The out-group homogeneity effect has been found using a wide variety of different social groups, from political and racial groups to age and gender groups.[4]\n", "The out-group homogeneity effect is part of a broader field of research that examines perceived group variability.[5] This area includes in-group homogeneity effects as well as out-group homogeneity effects, and it also deals with perceived group variability effects that are not linked to in-group\/out-group membership, such as effects that are related to the power, status, and size of groups. The out-group homogeneity effect has been found using a wide variety of different social groups, from political and racial groups to age and gender groups.[4] The implications of this effect on stereotyping have been noted.[6]\n"]},
{"name": "Cross-race effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Cross-race_effect", "ptags": ["The cross-race effect (sometimes called cross-race bias, other-race bias, own-race bias or other-race effect) is the tendency to more easily recognize faces that belong to one's own racial group, or racial groups that one has been in contact with. In social psychology, the cross-race effect is described as the \"ingroup advantage,\" whereas in other fields, the effect can be seen as a specific form of the \"ingroup advantage\" since it is only applied in interracial or inter-ethnic situations.[1] The cross-race effect is thought to contribute to difficulties in cross-race identification, as well as implicit racial bias.[2]\n", "A number of theories as to why the cross-race effect exists have been conceived, including social cognition and perceptual expertise. However, no model has been able to fully account for the full body of evidence. [3]\n"]},
{"name": "In-group bias", "link": "https:\/\/en.wikipedia.org\/wiki\/In-group_favoritism", "ptags": ["\n", "In-group favoritism, sometimes known as in-group\u2013out-group bias, in-group bias, intergroup bias, or in-group preference, is a pattern of favoring members of one's in-group over out-group members. This can be expressed in evaluation of others, in allocation of resources, and in many other ways.[1][2]\n", "This effect has been researched by many psychologists and linked to many theories related to group conflict and prejudice. The phenomenon is primarily viewed from a social psychology standpoint. Studies have shown that in-group favoritism arises as a result of the formation of cultural groups.[3][4] These cultural groups can be divided based on seemingly trivial observable traits, but with time, populations grow to associate certain traits with certain behavior, increasing covariation. This then incentivizes in-group bias.\n", "Two prominent theoretical approaches to the phenomenon of in-group favoritism are realistic conflict theory and social identity theory. Realistic conflict theory proposes that intergroup competition, and sometimes intergroup conflict, arises when two groups have opposing claims to scarce resources. In contrast, social identity theory posits a psychological drive for positively distinct social identities as the general root cause of in-group favoring behavior.\n"]},
{"name": "Halo effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Halo_effect", "ptags": ["The halo effect (sometimes called the halo error) is the tendency for positive impressions of a person, company, country, brand, or product in one area to positively influence one's opinion or feelings.[1][2] Halo effect is \u201dthe name given to the phenomenon whereby evaluators tend to be influenced by their previous judgments of performance or personality.\u201d[3] The halo effect is a cognitive bias which can prevent someone from forming an image of a person, a product or a brand based on the sum of all objective circumstances at hand.\n", "The term was coined by Edward Thorndike. A simplified example of the halo effect is a person, after noticing that an individual in a photograph is attractive, well groomed, and properly attired, then assuming, using a mental heuristic, that the person in the photograph is a good person based upon the rules of their own social concept.[4][5][6] This constant error in judgment is reflective of the individual's preferences, prejudices, ideology, aspirations, and social perception.[7][6][8][9][10]\n"]},
{"name": "Cheerleader effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Cheerleader_effect", "ptags": ["The cheerleader effect, also known as the group attractiveness effect or the friend effect,[1] is a proposed cognitive bias which causes people to perceive individuals as 1.5\u20132.0% more attractive in a group than when seen alone.[2] The first paper to report this effect was written by Drew Walker and Edward Vul, in 2013.[3]\n", "Physical attractiveness implies individuals' preferences in a sexual selection based on the evolutionary psychology. In 1979, Donald Symons first proposed this evolutionary explanation, suggesting that the evolving physical attractiveness results from mate assessment favoring partners who exhibited signs of good health and fertility, including face averageness.[4] This preference was proved to be shared across cultures.[5] Two parts constitute physical attractiveness, and most former studies investigated underlying mechanisms leading to cheerleader effect specifically in its subset, facial attractiveness.[1][2][5] Nevertheless, a study has recognized this effect in another physical appearance indicator, human body perceptions.[6]\n", "The effect size of the cheerleader effect is not modulated by the presentation time,[2] the number of individuals surrounding the target,[3] spatial arrangement of the faces in the group.[7] However, another study argued that the arrangement of faces in the group might influence this effect since people's central viewing tendency might affect observers to focus more on the perceived attractiveness of the middle face in the group.[8]\n", "Findings of this effect are interdisciplinary in applications. Based on them, mate choice,[9] marketing,[10] and social media[11] tactics are designed to increase the attractiveness of a target individual or item via the help of the group.\n"]},
{"name": "Positivity effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Positivity_effect", "ptags": ["The positivity effect is the ability to constructively analyze a situation where the desired results are not achieved, but still obtain positive feedback that assists one's future progression.\n", "Empirical research findings suggest that the positivity effect can be influenced by internal positive speech, where engaging in constructive self-dialogue can significantly improve one\u2019s ability to perceive and react to challenging situations more optimistically.[1]\n", "The findings of a study show that the optimism bias in future-oriented thinking fulfils a self-improvement purpose while also suggesting this bias probably reflects a common underpinning motivational process across various future-thinking domains, either episodic or semantic.[2]\n"]},
{"name": "Not invented here", "link": "https:\/\/en.wikipedia.org\/wiki\/Not_invented_here", "ptags": ["Not invented here (NIH) is the tendency to avoid using or buying products, research, standards, or knowledge from external origins. It is usually adopted by social, corporate, or institutional cultures. Research illustrates a strong bias against ideas from the outside.[1]\n", "The reasons for not wanting to use the work of others are varied, but can include a desire to support a local economy instead of paying royalties to a foreign license-holder, fear of patent infringement, lack of understanding of the foreign work, an unwillingness to acknowledge or value the work of others, jealousy, belief perseverance, or forming part of a wider turf war.[2] As a social phenomenon, this tendency can manifest itself as an unwillingness to adopt an idea or product because it originates from another culture, a form of tribalism[3] and\/or an inadequate effort in choosing the right approach for the business.[4]\n", "The term is typically used in a pejorative sense. The opposite predisposition is sometimes called \"proudly found elsewhere\" (PFE)[5] or \"invented elsewhere\".\n"]},
{"name": "Reactive devaluation", "link": "https:\/\/en.wikipedia.org\/wiki\/Reactive_devaluation", "ptags": ["Reactive devaluation is a cognitive bias that occurs when a proposal is devalued if it appears to originate from an antagonist. The bias was proposed by Lee Ross and Constance Stillinger (1988).[1]\n", "Reactive devaluation could be caused by loss aversion or attitude polarization,[2] or na\u00efve realism.[3]\n"]},
{"name": "Well-traveled road effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Well_travelled_road_effect", "ptags": ["The well travelled road effect is a cognitive bias in which travellers will estimate the time taken to traverse routes differently depending on their familiarity with the route. Frequently travelled routes are assessed as taking a shorter time than unfamiliar routes.[1][2] This effect creates errors when estimating the most efficient route to an unfamiliar destination, when one candidate route includes a familiar route, whilst the other candidate route includes no familiar routes. The effect is most salient when subjects are driving, but is still detectable for pedestrians and users of public transport. The effect has been observed for centuries but was first studied scientifically in the 1980s and 1990s following from earlier \"heuristics and biases\" work undertaken by Daniel Kahneman and Amos Tversky.[3]\n", "Much like the Stroop task,[4] it is hypothesised that drivers use less cognitive effort when traversing familiar routes and therefore underestimate the time taken to traverse the familiar route.[5]  The well travelled road effect has been hypothesised as a reason that self-reported experience curve effects are overestimated.[citation needed]\n"]}
   
        ]
       },
   
       {
       "name": "We simplify probabilities and numbers to make them easier to think about",
        "children": [
   
{"name": "Mental accounting", "link": "https:\/\/en.wikipedia.org\/wiki\/Mental_accounting", "ptags": ["Mental accounting (or psychological accounting) is a model of consumer behaviour developed by Richard Thaler that attempts to describe the process whereby people code, categorize and evaluate economic outcomes.[2] Mental accounting incorporates the economic concepts of prospect theory and transactional utility theory to evaluate how people create distinctions between their financial resources in the form of mental accounts, which in turn impacts the buyer decision process and reaction to economic outcomes. [3] People are presumed to make mental accounts as a self control strategy to manage and keep track of their spending and resources.[4] People budget money into mental accounts for savings (e.g., saving for a home) or expense categories (e.g., gas money, clothing, utilities).[5] People also are assumed to make mental accounts to facilitate savings for larger purposes (e.g., a home or college tuition).[6] Mental accounting can result in people demonstrating greater loss aversion for certain mental accounts, resulting in cognitive bias that incentivizes systematic departures from consumer rationality. Through increased understanding of mental accounting differences in decision making based on different resources, and different reactions based on similar outcomes can be greater understood.  \n", "As Thaler puts it, \u201cAll organizations, from General Motors down to single person households, have explicit and\/or implicit accounting systems. The accounting system often influences decisions in unexpected ways\u201d.[7] Particularly, individual expenses will usually not be considered in conjunction with the present value of one\u2019s total wealth; they will be instead considered in the context of two accounts: the current budgetary period (this could be a monthly process due to bills, or yearly due to an annual income), and the category of expense. [8] People can even have multiple mental accounts for the same kind of resource. A person may use different monthly budgets for grocery shopping and eating out at restaurants, for example, and constrain one kind of purchase when its budget has run out while not constraining the other kind of purchase, even though both expenditures draw on the same fungible resource (income).[9]\n", "One detailed application of mental accounting, the Behavioral Life Cycle Hypothesis posits that people mentally frame assets as belonging to either current income, current wealth or future income and this has implications for their behavior as the accounts are largely non-fungible and marginal propensity to consume out of each account is different. [10]\n"]},
{"name": "Appeal to probability fallacy", "link": "https:\/\/en.wikipedia.org\/wiki\/Appeal_to_probability", "ptags": ["An appeal to probability (or appeal to possibility, also known as possibiliter ergo probabiliter, \"possibly, therefore probably\") is the logical fallacy of taking something for granted because it is possibly the case.[1][2]  The fact that an event is possible does not imply that the event is probable, nor that the event was realized. \n"]},
{"name": "Normalcy bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Normalcy_bias", "ptags": ["Normalcy bias, or normality bias, is a cognitive bias which leads people to disbelieve or minimize threat warnings.[1] Consequently, individuals underestimate the likelihood of a disaster, when it might affect them, and its potential adverse effects.[2] The normalcy bias causes many people to prepare inadequately for natural disasters, market crashes, and calamities caused by human error. About 80% of people reportedly display normalcy bias during a disaster.[3]\n", "The normalcy bias can manifest in response to warnings about disasters and actual catastrophes. Such events can range in scale from incidents such as traffic collisions to global catastrophic risk. The event may involve social constructionism phenomena such as loss of money in market crashes, or direct threats to continuity of life: as in natural disasters like a tsunami or violence in war.\n", "Normalcy bias has also been called analysis paralysis, the ostrich effect,[4] and by first responders, the negative panic.[5] The opposite of normalcy bias is overreaction, or worst-case scenario bias,[6][7] in which small deviations from normality are dealt with as signals of an impending catastrophe.\n"]},
{"name": "Murphy's Law", "link": "https:\/\/en.wikipedia.org\/wiki\/Murphy's_law", "ptags": ["Murphy's law[a] is an adage or epigram that is typically stated as: \"Anything that can go wrong will go wrong.\" In some formulations, it is extended to \"Anything that can go wrong will go wrong, and at the worst possible time.\"\n", "Though similar statements and concepts have been made over the course of history, the law itself was coined by, and is named after, American aerospace engineer Edward A. Murphy Jr.; its exact origins are debated, but it is generally agreed it originated from Murphy and his team following a mishap during rocket sled tests some time between 1948 and 1949, and was finalized and first popularized by testing project head John Stapp during a later press conference. Murphy's original quote was the precautionary design advice that \"If there are two or more ways to do something and one of those results in a catastrophe, then someone will do it that way.\"[1][2]\n", "The law entered wider public knowledge in the late 1970s with the publication of Arthur Bloch's 1977 book Murphy's Law, and Other Reasons Why Things Go WRONG, which included other variations and corollaries of the law. Since then, Murphy's law has remained a popular (and occasionally misused) adage, though its accuracy has been disputed by academics. \n", "Murphy's law is simply an adage and is not grounded on logic or scientific laws. Similar \"laws\" include Sod's law, Finagle's law, and Yhprum's law, among others.\n"]},
{"name": "Zero sum bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Zero-sum_thinking", "ptags": ["Zero-sum thinking perceives situations as zero-sum games, where one person's gain would be another's loss.[1][2][3] The term is derived from game theory. However, unlike the game theory concept, zero-sum thinking refers to a psychological construct\u2014a person's subjective interpretation of a situation. Zero-sum thinking is captured by the saying \"your gain is my loss\" (or conversely, \"your loss is my gain\"). Rozycka-Tran et al. (2015) defined zero-sum thinking as:\n", "Zero-sum bias is a cognitive bias towards zero-sum thinking; it is people's tendency to intuitively judge that a situation is zero-sum, even when this is not the case.[4] This bias promotes zero-sum fallacies, false beliefs that situations are zero-sum. Such fallacies can cause other false judgements and poor decisions.[5][6] In economics, \"zero-sum fallacy\" generally refers to the fixed-pie fallacy.\n"]},
{"name": "Survivorship bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Survivorship_bias", "ptags": ["\n", "Survivorship bias or survival bias is the logical error of concentrating on entities that passed a selection process while overlooking those that did not. This can lead to incorrect conclusions because of incomplete data.  \n", "Survivorship bias is a form of selection bias that can lead to overly optimistic beliefs because multiple failures are overlooked, such as when companies that no longer exist are excluded from analyses of financial performance. It can also lead to the false belief that the successes in a group have some special property, rather than just coincidence as in correlation \"proves\" causality.\n", "Another kind of survivorship bias would involve thinking that an incident happened in a particular way because the only people who were involved in the incident who can speak about it are those who survived it. Even if one knew that some people are dead, they would not have their voice to add to the conversation, making it biased.\n"]},
{"name": "Subadditivity effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Subadditivity_effect", "ptags": ["The subadditivity effect is the tendency to judge probability of the whole to be less than the probabilities of the parts.[1]\n"]},
{"name": "Denomination effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Denomination_effect", "ptags": ["\n", "The denomination effect is a form of cognitive bias relating to currency, suggesting people may be less likely to spend larger currency denominations than their equivalent value in smaller denominations.[1] It was proposed by Priya Raghubir, professor at the New York University Stern School of Business, and Joydeep Srivastava, professor at University of Maryland, in their 2009 paper \"Denomination Effect\".[2][3]\n", "Raghubir and Srivastava conducted three studies in their research on the denomination effect; their findings suggested people may be more likely to spend money represented by smaller denominations and that consumers may prefer to receive money in a large denomination when there is a need to control spending. The denomination effect can occur when large denominations are perceived as less exchangeable than smaller denominations.\n", "The effect's influence on spending decisions has implications throughout various sectors in society, including consumer welfare, monetary policy and the finance industry. For example, during the Great Recession, one businessman observed employees using more coins rather than banknotes in an office vending machine, perceiving the customers used coins to feel thriftier. Raghubir and Srivastava also suggested the effect may involve incentives to alter future behavior and that a large denomination can serve as a mechanism to prevent the urge to spend.\n"]},
{"name": "Magic number 7+-2", "link": "https:\/\/en.wikipedia.org\/wiki\/The_Magical_Number_Seven,_Plus_or_Minus_Two", "ptags": ["\n", "\"The Magical Number Seven, Plus or Minus Two: Some Limits on Our Capacity for Processing Information\"[1] is one of the most highly cited papers in psychology.[2][3][4] It was written by the cognitive psychologist George A. Miller of Harvard University's Department of Psychology and published in 1956 in Psychological Review. It is often interpreted to argue that the number of objects an average human can hold in short-term memory is 7 \u00b1 2. This has occasionally been referred to as Miller's law.[5][6][7]\n"]}
   
        ]
       },
   
       {
       "name":"We think we know what other people are thinking",
        "children": [
   
{"name": "Illusion of transparency", "link": "https:\/\/en.wikipedia.org\/wiki\/Illusion_of_transparency", "ptags": ["The illusion of transparency is a tendency for people to overestimate the degree to which their personal mental state is known by others.[1] Another manifestation of the illusion of transparency (sometimes called the observer's illusion of transparency) is a tendency for people to overestimate how well they understand others' personal mental states. This cognitive bias is similar to the illusion of asymmetric insight.\n"]},
{"name": "Curse of knowledge", "link": "https:\/\/en.wikipedia.org\/wiki\/Curse_of_knowledge", "ptags": ["The curse of knowledge is a cognitive bias that occurs when an individual, who is communicating with others, assumes that others have information that is only available to themselves, assuming they all share a background and understanding.[1] This bias is also called by some authors the curse of expertise.[2]\n", "For example, in a classroom setting, teachers may have difficulty if they cannot put themselves in the position of the student. A knowledgeable professor might no longer remember the difficulties that a young student encounters when learning a new subject for the first time. This curse of knowledge also explains the danger behind thinking about student learning based on what appears best to faculty members, as opposed to what has been verified with students.[3]\n"]},
{"name": "Spotlight effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Spotlight_effect", "ptags": ["The spotlight effect is the psychological phenomenon by which people tend to believe they are being noticed more than they really are. Being that one is constantly in the center of one's own world, an accurate evaluation of how much one is noticed by others is uncommon. The reason for the spotlight effect is the innate tendency to forget that although one is the center of one's own world, one is not the center of everyone else's. This tendency is especially prominent when one does something atypical.[1]\n", "Research has empirically shown that such drastic over-estimation of one's effect on others is widely common. Many professionals in social psychology encourage people to be conscious of the spotlight effect and to allow this phenomenon to moderate the extent to which one believes one is in a social spotlight.[2]\n"]},
{"name": "Extrinsic incentive error", "link": "https:\/\/en.wikipedia.org\/wiki\/Extrinsic_incentives_bias", "ptags": ["The extrinsic incentives bias is an attributional bias according to which people attribute relatively more to \"extrinsic incentives\" (such as monetary reward) than to \"intrinsic incentives\" (such as learning a new skill) when weighing the motives of others rather than themselves.\n", "It is a counter-example to the fundamental attribution error as according to the extrinsic bias others are presumed to have situational motivations while oneself is seen as having dispositional motivations. This is the opposite of what the fundamental attribution error would predict. It also might help to explain some of the backfiring effects that can occur when extrinsic incentives are attached to activities that people are intrinsically motivated to do. The term was first proposed by Chip Heath, citing earlier research by others in management science.[1]\n"]},
{"name": "Illusion of asymmetric insight", "link": "https:\/\/en.wikipedia.org\/wiki\/Illusion_of_asymmetric_insight", "ptags": ["The illusion of asymmetric insight is a cognitive bias whereby people perceive their knowledge of others to surpass other people's knowledge of them.[1] This bias \"has been traced to people's tendency to view their own spontaneous or off-the-cuff responses to others' questions as relatively unrevealing even though they view others' similar responses as meaningful\".[2]\n"]}
   
        ]
       },
   
       {
       "name": "We project our current mindset and assumptions onto the past and future",
        "children": [
   
{"name": "Telescoping effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Telescoping_effect", "ptags": ["In cognitive psychology, the telescoping effect (or telescoping bias) refers to the temporal displacement of an event whereby people perceive recent events as being more remote than they are and distant events as being more recent than they are.[1] The former is known as backward telescoping or time expansion, and the latter as is known as forward telescoping.[1]\n", "The approximate time frame in which events switch from being displaced backward in time to forward in time is three years, with events occurring three years in the past being equally likely to be reported with forward telescoping bias as with backward telescoping bias.[1] Although telescoping occurs in both the forward and backward directions, in general the effect is to increase the number of events reported too recently.[2] This net effect in the forward direction is because of forces that impair memory, such as lack of salience, also impair time perception.[2]\n", "Telescoping leads to an over-reporting of the frequency of events.[3] This over-reporting is because participants include events beyond the period, either events that are too recent for the target time period (backward telescoping) or events that are too old for the target time period (forward telescoping).[3]\n"]},
{"name": "Declinism", "link": "https:\/\/en.wikipedia.org\/wiki\/Declinism", "ptags": ["Declinism is the belief that a society or institution is tending towards decline. Particularly, it is the predisposition, caused by cognitive biases such as rosy retrospection, to view the past more favourably and the future more negatively.[1][2][3]\n", "\"The great summit of declinism\" according to Adam Gopnick, \"was established in 1918, in the book that gave decline its good name in publishing: the German historian Oswald Spengler's best-selling, thousand-page work The Decline of the West.\"[4]\n"]},
{"name": "Hindsight bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Hindsight_bias", "ptags": ["\n", "Hindsight bias, also known as the knew-it-all-along phenomenon[1] or creeping determinism,[2] is the common tendency for people to perceive past events as having been more predictable than they were.[3][4]\n", "After an event has occurred, people often believe that they could have predicted or perhaps even known with a high degree of certainty what the outcome of the event would be before it occurred. Hindsight bias may cause distortions of memories of what was known or believed before an event occurred and is a significant source of overconfidence in one\u2019s ability to predict the outcomes of future events.[5] Examples of hindsight bias can be seen in the writings of historians describing the outcomes of battles, in physicians\u2019 recall of clinical trials, and in criminal or civil trials as people tend to assign responsibility on the basis of the supposed predictability of accidents.[6][7][2]\n", "In some countries, 20\/20 indicates normal visual acuity at 20 feet, from which derives the idiom \"hindsight is 20\/20\".\n"]},
{"name": "Outcome bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Outcome_bias", "ptags": ["The outcome bias is an error made in evaluating the quality of a decision when the outcome of that decision is already known. Specifically, the outcome effect occurs when the same \"behavior produce[s] more ethical condemnation when it happen[s] to produce bad rather than good outcome, even if the outcome is determined by chance.\"[1]\n", "While similar to the hindsight bias, the two phenomena are markedly different. Hindsight bias focuses on memory distortion to favor the actor, while the outcome bias focuses exclusively on weighting the outcome heavier than other pieces of information in deciding if a past decision was correct.\n"]},
{"name": "Moral luck", "link": "https:\/\/en.wikipedia.org\/wiki\/Moral_luck", "ptags": ["Moral luck describes circumstances whereby a moral agent is assigned moral blame or praise for an action or its consequences, even if it is clear that said agent did not have full control over either the action or its consequences. This term, introduced by Bernard Williams, has been developed, along with its significance to a coherent moral theory, by Williams and Thomas Nagel in their respective essays on the subject.\n"]},
{"name": "Declinism", "link": "https:\/\/en.wikipedia.org\/wiki\/Declinism", "ptags": ["Declinism is the belief that a society or institution is tending towards decline. Particularly, it is the predisposition, caused by cognitive biases such as rosy retrospection, to view the past more favourably and the future more negatively.[1][2][3]\n", "\"The great summit of declinism\" according to Adam Gopnick, \"was established in 1918, in the book that gave decline its good name in publishing: the German historian Oswald Spengler's best-selling, thousand-page work The Decline of the West.\"[4]\n"]},
{"name": "Impact bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Impact_bias", "ptags": ["In the psychology of affective forecasting, the impact bias, a form of which is the durability bias, is the tendency for people to overestimate the length or the intensity of future emotional states.[1]\n"]},
{"name": "Optimism bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Optimism_bias", "ptags": ["Optimism bias (or the optimistic bias) is a cognitive bias that causes someone to believe that they themselves are less likely to experience a negative event. It is also known as delusional optimism, unrealistic optimism or comparative optimism.\n", "Optimism bias is common and transcends gender, ethnicity, nationality, and age.[1] Optimistic biases are even reported in animals such as rats and birds.[2] However, autistic people are less susceptible to optimistic biases.[3]\n", "Four factors can cause a person to be optimistically biased: their desired end state, their cognitive mechanisms, the information they have about themselves versus others, and overall mood.[4] The optimistic bias is seen in a number of situations. For example: people believing that they are less at risk of being a crime victim,[5] smokers believing that they are less likely to contract lung cancer or disease than other smokers, first-time bungee jumpers believing that they are less at risk of an injury than other jumpers,[6] or traders who think they are less exposed to potential losses in the markets.[7]\n", "Although the optimism bias occurs for both positive events (such as believing oneself to be more financially successful than others) and negative events (such as being less likely to have a drinking problem), there is more research and evidence suggesting that the bias is stronger for negative events (the valence effect).[4][8] Different consequences result from these two types of events: positive events often lead to feelings of well being and self-esteem, while negative events lead to consequences involving more risk, such as engaging in risky behaviors and not taking precautionary measures for safety.[4]\n"]},
{"name": "Planning fallacy", "link": "https:\/\/en.wikipedia.org\/wiki\/Planning_fallacy", "ptags": ["The planning fallacy is a phenomenon in which predictions about how much time will be needed to complete a future task display an optimism bias and underestimate the time needed. This phenomenon sometimes occurs regardless of the individual's knowledge that past tasks of a similar nature have taken longer to complete than generally planned.[1][2][3] The bias affects predictions only about one's own tasks. On the other hand, when outside observers predict task completion times, they tend to exhibit a pessimistic bias, overestimating the time needed.[4][5] The planning fallacy involves estimates of task completion times more optimistic than those encountered in similar projects in the past.\n", "The planning fallacy was first proposed by Daniel Kahneman and Amos Tversky in 1979.[6][7] In 2003, Lovallo and Kahneman proposed an expanded definition as the tendency to underestimate the time, costs, and risks of future actions and at the same time overestimate the benefits of the same actions. According to this definition, the planning fallacy results in not only time overruns, but also cost overruns and benefit shortfalls.[8]\n"]},
{"name": "Time-saving bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Time-saving_bias", "ptags": ["Time-saving bias is a concept that describes people's tendency to misestimate the time that could be saved (or lost) when increasing (or decreasing) speed.[1][2]\n", "In general, people underestimate the time that could be saved when increasing from a relatively low speed\u2014e.g., 25\u00a0mph (40\u00a0km\/h) or 40\u00a0mph (64\u00a0km\/h)\u2014and overestimate the time that could be saved when increasing from a relatively high speed\u2014e.g., 55\u00a0mph (89\u00a0km\/h) or 90\u00a0mph (140\u00a0km\/h). People also underestimate the time that could be lost when decreasing from a low speed and overestimate the time that could be lost when decreasing from a high speed.\n"]},
{"name": "Pro-innovation bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Pro-innovation_bias", "ptags": ["In diffusion of innovation theory, a pro-innovation bias is a belief that innovation should be adopted by the whole society without the need for its alteration.[1][2] The innovation's \"champion\" has a such strong bias in favor of the innovation, that they may not see its limitations or weaknesses and continue to promote it nonetheless.[3]\n"]},
{"name": "Projection bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Affective_forecasting#Projection_bias", "ptags": ["\nAffective forecasting, also known as hedonic forecasting or the hedonic forecasting mechanism, is the prediction of one's affect (emotional state) in the future.[1] As a process that influences preferences, decisions, and behavior, affective forecasting is studied by both psychologists and economists, with broad applications.\n"]},
{"name": "Restraint bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Restraint_bias", "ptags": ["Restraint bias is the tendency for people to overestimate their ability to control impulsive behavior. An inflated self-control belief may lead to greater exposure to temptation, and increased impulsiveness. Therefore, the restraint bias has bearing on addiction. For example, someone might use drugs, simply because they believe they can resist any potential addiction.[1]\nAn individual's inability to control, or their temptation can come from several different  visceral impulses. Visceral impulses can include hunger, sexual arousal, and fatigue. These impulses provide information about the current state and behavior needed to keep the body satisfied.[1]\n", "Empathy Gap Effect:\nThe Empathy Gap Effect deals with individuals having trouble appreciating the power that the impulse states have on their behavior. There is a cold-to-hot empathy gap that states when people are in a cold state, like not experiencing hunger, they tended to underestimate those influences in a hot state. The underestimation of the visceral impulses can be contributed to restricted memory for the visceral experience which means the individual can recall the impulsive state but cannot recreate the sensation of the impulsive state.[1]\n", "Impulse Control and Attention:\nStudies have concluded that when people believe that they have stronger sense of self-control over situations in their environment, they have greater impulse control. Individuals also tend to overestimate their capacity for self-control when one is told that they have a high capacity for self-restraint.[1] The more someone is told that they have a high capacity for self-restraint, the more they believe it and display higher levels of impulse control. Attention has a lot to do with biases, self and impulse controls in our environment. The less attention an individual pays to something, the less control they have over whatever they are doing. Focusing attention to oneself can lead to successful self-control which can be helpful in many aspects of life. Self-control engages conflict between competing pressures, pressures that can be brought on by situational or internal prompts from the environment. Some of the cues make the individual act on or engage in that behavior or act to prevent the individual from taking action.[2]\n"]},
{"name": "Self-consistency bias", "link": "https:\/\/en.wikipedia.org\/wiki\/List_of_cognitive_biases#Consistency_bias", "ptags": ["Cognitive biases are systematic patterns of deviation from norm and\/or rationality in judgment. They are often studied in psychology, sociology and behavioral economics.[1]\n", "Although the reality of most of these biases is confirmed by reproducible research,[2][3] there are often controversies about how to classify these biases or how to explain them.[4] Several theoretical causes are known for some cognitive biases, which provides a classification of biases by their common generative mechanism (such as noisy information-processing[5]). Gerd Gigerenzer has criticized the framing of cognitive biases as errors in judgment, and favors interpreting them as arising from rational deviations from logical thought.[6]\n", "Explanations include information-processing rules (i.e., mental shortcuts), called heuristics, that the brain uses to produce decisions or judgments. Biases have a variety of forms and appear as cognitive (\"cold\") bias, such as mental noise,[5] or motivational (\"hot\") bias, such as when beliefs are distorted by wishful thinking. Both effects can be present at the same time.[7][8]\n", "There are also controversies over some of these biases as to whether they count as useless or irrational, or whether they result in useful attitudes or behavior. For example, when getting to know others, people tend to ask leading questions which seem biased towards confirming their assumptions about the person. However, this kind of confirmation bias has also been argued to be an example of social skill; a way to establish a connection with the other person.[9]\n", "Although this research overwhelmingly involves human subjects, some findings that demonstrate bias have been found in non-human animals as well. For example, loss aversion has been shown in monkeys and hyperbolic discounting has been observed in rats, pigeons, and monkeys.[10]\n"]}
   
        ]
       }
   
      ]
     },
   
     {
      "name": "3. Need To Act Fast",
      "children": [
   
       {
        "name": "To act, we must be confident we can make an impact and feel what we do is important",
        "children": [
   
{"name": "Overconfidence effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Overconfidence_effect", "ptags": ["The overconfidence effect is a well-established bias in which a person's subjective confidence in their judgments is reliably greater than the objective accuracy of those judgments, especially when confidence is relatively high.[1][2] Overconfidence is one example of a miscalibration of subjective probabilities. Throughout the research literature, overconfidence has been defined in three distinct ways: (1) overestimation of one's actual performance; (2) overplacement of one's performance relative to others; and (3) overprecision in expressing unwarranted certainty in the accuracy of one's beliefs.[3][4]\n", "The most common way in which overconfidence has been studied is by asking people how confident they are of specific beliefs they hold or answers they provide.  The data show that confidence systematically exceeds accuracy, implying people are more sure that they are correct than they deserve to be. If human confidence had perfect calibration, judgments with 100% confidence would be correct 100% of the time, 90% confidence correct 90% of the time, and so on for the other levels of confidence. By contrast, the key finding is that confidence exceeds accuracy so long as the subject is answering hard questions about an unfamiliar topic. For example, in a spelling task, subjects were correct about 80% of the time, whereas they claimed to be 100% certain.[5] Put another way, the error rate was 20% when subjects expected it to be 0%.  In a series where subjects made true-or-false responses to general knowledge statements, they were overconfident at all levels. When they were 100% certain of their answer to a question, they were wrong 20% of the time.[6]\n"]},
{"name": "Social desirability bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Social_desirability_bias", "ptags": ["In social science research, social-desirability bias is a type of response bias that is the tendency of survey respondents to answer questions in a manner that will be viewed favorably by others.[1] It can take the form of over-reporting \"good behavior\" or under-reporting \"bad\", or undesirable behavior. The tendency poses a serious problem with conducting research with self-reports. This bias interferes with the interpretation of average tendencies as well as individual differences.\n"]},
{"name": "Third-person effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Third-person_effect", "ptags": ["The third-person effect [1] hypothesis predicts that people tend to perceive that mass media messages have a greater effect on others than on themselves, based on personal biases. The third-person effect manifests itself through an individual's overestimation of the effect of a mass communicated message on the generalized other, or an underestimation of the effect of a mass communicated message on themselves.\n", "These types of perceptions stem from a self-motivated social desirability (not feeling influenced by mass messages promotes self-esteem), a social-distance corollary (choosing to dissociate oneself from the others who may be influenced), and a perceived exposure to a message (others choose to be influenced by persuasive communication).[1] Other names for the effect are \"Third-person perception\" and \"Web Third-person effect\". From 2015, the effect is named \"Web Third-person effect\" when it is verified in social media, media websites, blogs and in websites in general.[2]\n"]},
{"name": "False consensus effect", "link": "https:\/\/en.wikipedia.org\/wiki\/False_consensus_effect", "ptags": ["In psychology, the false consensus effect, also known as consensus bias, is a pervasive cognitive bias that causes people to \"see their own behavioral choices and judgments as relatively common and appropriate to existing circumstances\".[1] In other words, they assume that their personal qualities, characteristics, beliefs, and actions are relatively widespread through the general population.\n", "This false consensus is significant because it increases self-esteem (overconfidence effect). It can be derived from a desire to conform and be liked by others in a social environment. This bias is especially prevalent in group settings where one thinks the collective opinion of their own group matches that of the larger population. Since the members of a group reach a consensus and rarely encounter those who dispute it, they tend to believe that everybody thinks the same way. The false-consensus effect is not restricted to cases where people believe that their values are shared by the majority, but it still manifests as an overestimate of the extent of their belief.[2]\n", "Additionally, when confronted with evidence that a consensus does not exist, people often assume that those who do not agree with them are defective in some way.[3] There is no single cause for this cognitive bias; the availability heuristic, self-serving bias, and na\u00efve realism have been suggested as at least partial underlying factors. The bias may also result, at least in part, from non-social stimulus-reward associations.[4] Maintenance of this cognitive bias may be related to the tendency to make decisions with relatively little information. When faced with uncertainty and a limited sample from which to make decisions, people often \"project\" themselves onto the situation. When this personal knowledge is used as input to make generalizations, it often results in the false sense of being part of the majority.[5]\n", "The false consensus effect has been widely observed and supported by empirical evidence. Previous research has suggested that cognitive and perceptional factors (motivated projection, accessibility of information, emotion, etc.) may contribute to the consensus bias, while recent studies have focused on its neural mechanisms. One recent study has shown that consensus bias may improve decisions about other people's preferences.[4] Ross, Green and House first defined the false consensus effect in 1977 with emphasis on the relative commonness that people perceive about their own responses; however, similar projection phenomena had already caught attention in psychology. Specifically, concerns with respect to connections between individual's personal predispositions and their estimates of peers appeared in the literature for a while. For instances, Katz and Allport in 1931 illustrated that students\u2019 estimates of the amount of others on the frequency of cheating was positively correlated to their own behavior. Later, around 1970, same phenomena were found on political beliefs and prisoner's dilemma situation. In 2017, researchers identified a persistent egocentric bias when participants learned about other people's snack-food preferences.[4] Moreover, recent studies suggest that the false consensus effect can also affect professional decision makers; specifically, it has been shown that even experienced marketing managers project their personal product preferences onto consumers.[6][7]\n"]},
{"name": "Hard-easy effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Hard\u2013easy_effect", "ptags": ["\nThe hard\u2013easy effect is a cognitive bias that manifests itself as a tendency to overestimate the probability of one's success at a task perceived as hard, and to underestimate the likelihood of one's success at a task perceived as easy. The hard-easy effect takes place, for example, when individuals exhibit a degree of underconfidence in answering relatively easy questions and a degree of overconfidence in answering relatively difficult questions. \"Hard tasks tend to produce overconfidence but worse-than-average perceptions,\" reported Katherine A. Burson, Richard P. Larrick, and Jack B. Soll in a 2005 study, \"whereas easy tasks tend to produce underconfidence and better-than-average effects.\"[1]\n", "The hard-easy effect falls under the umbrella of \"social comparison theory\", which was originally formulated by Leon Festinger in 1954. Festinger argued that individuals are driven to evaluate their own opinions and abilities accurately, and social comparison theory explains how individuals carry out those evaluations by comparing themselves to others.[2]\n", "In 1980, Ferrell and McGoey called it the \"discriminability effect\"; in 1992, Griffin and Tversky called it the \"difficulty effect\".[3]\n"]},
{"name": "Lake Wobegone effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Lake_Wobegon#The_Lake_Wobegon_effect", "ptags": ["Lake Wobegon is a fictional town created by Garrison Keillor as the setting of the recurring segment \"News from Lake Wobegon\" for the radio program A Prairie Home Companion broadcast from St Paul, Minnesota. The fictional town serves as the setting for many of Keillor's stories and novels, gaining an international audience with Lake Wobegon Days in 1985. Described as a small rural town in central Minnesota, the events and adventures of the townspeople provided Keillor with a wealth of humorous and often touching stories.[1][2]\n", "Keillor has said that people often ask him if it is a real town, and when he replied that it was not, they seemed disappointed because \"people want stories to be true\". So he began to say it was in \"central Minnesota, near Stearns County, up around Holdingford, not far from St. Rosa and Albany and Freeport, northwest of St. Cloud\", which he says is \"sort of the truth, I guess.\"[3]\n"]},
{"name": "Dunning-Kruger effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Dunning\u2013Kruger_effect", "ptags": ["\n", "The Dunning\u2013Kruger effect is a cognitive bias in which people with limited competence in a particular domain overestimate their abilities. It was first described by Justin Kruger and David Dunning in 1999. Some researchers also include the opposite effect for high performers: their tendency to underestimate their skills. In popular culture, the Dunning\u2013Kruger effect is often misunderstood as a claim about general overconfidence of people with low intelligence instead of specific overconfidence of people unskilled at a particular task.\n", "Numerous similar studies have been done. The Dunning\u2013Kruger effect is usually measured by comparing self-assessment with objective performance. For example, participants may take a quiz and estimate their performance afterward, which is then compared to their actual results. The original study focused on logical reasoning, grammar, and social skills. Other studies have been conducted across a wide range of tasks. They include skills from fields such as business, politics, medicine, driving, aviation, spatial memory, examinations in school, and literacy.\n", "There is disagreement about the causes of the Dunning\u2013Kruger effect. According to the metacognitive explanation, poor performers misjudge their abilities because they fail to recognize the qualitative difference between their performances and the performances of others. The statistical model explains the empirical findings as a statistical effect in combination with the general tendency to think that one is better than average. Some proponents of this view hold that the Dunning\u2013Kruger effect is mostly a statistical artifact. The rational model holds that overly positive prior beliefs about one's skills are the source of false self-assessment. Another explanation claims that self-assessment is more difficult and error-prone for low performers because many of them have very similar skill levels.\n", "There is also disagreement about where the effect applies and about how strong it is, as well as about its practical consequences. Inaccurate self-assessment could potentially lead people to making bad decisions, such as choosing a career for which they are unfit, or engaging in dangerous behavior. It may also inhibit people from addressing their shortcomings to improve themselves. Critics argue that such an effect would have much more dire consequences than what is observed.\n"]},
{"name": "Egocentric bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Egocentric_bias", "ptags": ["Egocentric bias is the tendency to rely too heavily on one's own perspective and\/or have a higher opinion of oneself than reality.[1] It appears to be the result of the psychological need to satisfy one's ego and to be advantageous for memory consolidation. Research[by whom?] has shown [citation needed][weasel\u00a0words] that experiences, ideas, and beliefs are more easily recalled when they match one's own, causing an egocentric outlook. Michael Ross and Fiore Sicoly first identified this cognitive bias in their 1979 paper, \"Egocentric biases in availability and attribution\".[2][3] Egocentric bias is referred to by most psychologists as a general umbrella term under which other related phenomena fall.\n", "The effects of egocentric bias can differ based on personal characteristics, such as age and the number of languages one speaks.   Thus far, there have been many studies focusing on specific implications of egocentric bias in different contexts.  Research on collaborative group tasks have emphasized that people view their own contributions differently than they view that of others.  Other areas of research have been aimed at studying how mental health patients display egocentric bias, and at the relationship between egocentric bias and voter distribution.  These types of studies surrounding egocentric bias usually involve written or verbal questionnaires, based on the subject's personal life or their decision in various hypothetical scenarios.\n"]},
{"name": "Optimism bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Optimism_bias", "ptags": ["Optimism bias (or the optimistic bias) is a cognitive bias that causes someone to believe that they themselves are less likely to experience a negative event. It is also known as delusional optimism, unrealistic optimism or comparative optimism.\n", "Optimism bias is common and transcends gender, ethnicity, nationality, and age.[1] Optimistic biases are even reported in animals such as rats and birds.[2] However, autistic people are less susceptible to optimistic biases.[3]\n", "Four factors can cause a person to be optimistically biased: their desired end state, their cognitive mechanisms, the information they have about themselves versus others, and overall mood.[4] The optimistic bias is seen in a number of situations. For example: people believing that they are less at risk of being a crime victim,[5] smokers believing that they are less likely to contract lung cancer or disease than other smokers, first-time bungee jumpers believing that they are less at risk of an injury than other jumpers,[6] or traders who think they are less exposed to potential losses in the markets.[7]\n", "Although the optimism bias occurs for both positive events (such as believing oneself to be more financially successful than others) and negative events (such as being less likely to have a drinking problem), there is more research and evidence suggesting that the bias is stronger for negative events (the valence effect).[4][8] Different consequences result from these two types of events: positive events often lead to feelings of well being and self-esteem, while negative events lead to consequences involving more risk, such as engaging in risky behaviors and not taking precautionary measures for safety.[4]\n"]},
{"name": "Barnum effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Barnum_effect", "ptags": ["\n", "The Barnum effect, also called the Forer effect or, less commonly, the Barnum\u2013Forer effect, is a common psychological phenomenon whereby individuals give high accuracy ratings to descriptions of their personality that supposedly are tailored specifically to them, yet which are in fact vague and general enough to apply to a wide range of people.[1] This effect can provide a partial explanation for the widespread acceptance of some paranormal beliefs and practices, such as astrology, fortune telling, aura reading, and some types of personality tests.[1]\n", "Psychologist Bertram Forer originally named it the \"fallacy of personal validation\".[2] The term \"Barnum effect\" was coined in 1956 by psychologist Paul Meehl in his essay \"Wanted \u2013 A Good Cookbook\", because he relates the vague personality descriptions used in certain \"pseudo-successful\" psychological tests to those given by showman P. T. Barnum.[3][4]\n"]},
{"name": "Barnum effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Barnum_effect", "ptags": ["\n", "The Barnum effect, also called the Forer effect or, less commonly, the Barnum\u2013Forer effect, is a common psychological phenomenon whereby individuals give high accuracy ratings to descriptions of their personality that supposedly are tailored specifically to them, yet which are in fact vague and general enough to apply to a wide range of people.[1] This effect can provide a partial explanation for the widespread acceptance of some paranormal beliefs and practices, such as astrology, fortune telling, aura reading, and some types of personality tests.[1]\n", "Psychologist Bertram Forer originally named it the \"fallacy of personal validation\".[2] The term \"Barnum effect\" was coined in 1956 by psychologist Paul Meehl in his essay \"Wanted \u2013 A Good Cookbook\", because he relates the vague personality descriptions used in certain \"pseudo-successful\" psychological tests to those given by showman P. T. Barnum.[3][4]\n"]},
{"name": "Self-serving bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Self-serving_bias", "ptags": ["A self-serving bias is any cognitive or perceptual process that is distorted by the need to maintain and enhance self-esteem, or the tendency to perceive oneself in an overly favorable manner.[1] It is the belief that individuals tend to ascribe success to their own abilities and efforts, but ascribe failure to external factors.[2] When individuals reject the validity of negative feedback, focus on their strengths and achievements but overlook their faults and failures, or take more credit for their group's work than they give to other members, they are protecting their self-esteem from threat and injury. These cognitive and perceptual tendencies perpetuate illusions and error, but they also serve the self's need for esteem.[3]  For example, a student who attributes earning a good grade on an exam to their own intelligence and preparation but attributes earning a poor grade to the teacher's poor teaching ability or unfair test questions might be exhibiting a self-serving bias. Studies have shown that similar attributions are made in various situations, such as the workplace,[4] interpersonal relationships,[5] sports,[6] and consumer decisions.[7]\n", "Both motivational processes (i.e. self-enhancement, self-presentation) and cognitive processes (i.e. locus of control, self-esteem) influence the self-serving bias.[8] There are both cross-cultural (i.e. individualistic and collectivistic culture differences) and special clinical population (i.e. depression) considerations within the bias.[9][10] Much of the research on the self-serving bias has used participant self-reports of attribution based on experimental manipulation of task outcomes or in naturalistic situations.[2] Some more modern research, however, has shifted focus to physiological manipulations, such as emotional inducement and neural activation, in an attempt to better understand the biological mechanisms that contribute to the self-serving bias.[11][12]\n"]},
{"name": "Actor-observer bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Actor\u2013observer_asymmetry#bias", "ptags": ["Actor\u2013observer asymmetry (also actor\u2013observer bias) is a bias one makes when forming attributions about the behavior of others or themselves.[1] When people judge their own behavior, they are more likely to attribute their actions to the particular situation than to their personality. However, when an observer is explaining the behavior of another person, they are more likely to attribute this behavior to the actors' personality rather than to situational factors.\n", "Sometimes the actor\u2013observer asymmetry is defined as the fundamental attribution error,[2] which is when people tend to explain behavior on the internal, personal characteristics rather than the external factors or situational influences.[3]\n", "The specific hypothesis of an actor\u2013observer asymmetry in attribution was originally proposed by Edward Jones and Richard Nisbett, where they said that \"actors tend to attribute the causes of their behavior to stimuli inherent in the situation, while observers tend to attribute behavior to stable dispositions of the actor\".[1] Supported by initial evidence, the hypothesis was long held as firmly established. However, a meta-analysis of all the published tests of the hypothesis between 1971 and 2004 found that there was no actor\u2013observer asymmetry of the sort that had been previously proposed.[4] The author of the study interpreted this result not so much as proof that actors and observers explained behavior exactly the same way but as evidence that the original hypothesis was fundamentally flawed in the way it framed people's explanations of behavior as attributions to either stable dispositions or the situation.\n", "Considerations of actor\u2013observer differences can be found in other disciplines as well, such as philosophy (e.g. privileged access, incorrigibility), management studies, artificial intelligence, semiotics, anthropology, and political science.[5]\n"]},
{"name": "Illusion of control", "link": "https:\/\/en.wikipedia.org\/wiki\/Illusion_of_control", "ptags": ["The illusion of control is the tendency for people to overestimate their ability to control events. It was named by U.S. psychologist Ellen Langer and is thought to influence gambling behavior and belief in the paranormal.[1] Along with illusory superiority and optimism bias, the illusion of control is one of the positive illusions.\n"]},
{"name": "Illusory superiority", "link": "https:\/\/en.wikipedia.org\/wiki\/Illusory_superiority", "ptags": ["\n", "\n", "In social psychology, illusory superiority is a cognitive bias wherein people overestimate their own qualities and abilities compared to others. Illusory superiority is one of many positive illusions, relating to the self, that are evident in the study of intelligence, the effective performance of tasks and tests, and the possession of desirable personal characteristics and personality traits. Overestimation of abilities compared to an objective measure is known as the overconfidence effect.\n", "The term \"illusory superiority\" was first used by the researchers Van Yperen and Buunk, in 1991. The phenomenon is also known as the above-average effect, the superiority bias, the leniency error, the sense of relative superiority, the primus inter pares effect,[1] and the Lake Wobegon effect, named after the fictional town where all the children are above average.[2] The Dunning-Kruger effect is a form of illusory superiority shown by people on a task where their level of skill is low.\n", "\nA vast majority of the literature on illusory superiority originates from studies on participants in the United States. However, research that only investigates the effects in one specific population is severely limited as this may not be a true representation of human psychology. More recent research investigating self-esteem in other countries suggests that illusory superiority depends on culture.[3] Some studies indicate that East Asians tend to underestimate their own abilities in order to improve themselves and get along with others.[4][5]"]},
{"name": "Fundamental attribution error", "link": "https:\/\/en.wikipedia.org\/wiki\/Fundamental_attribution_error", "ptags": ["In social psychology, fundamental attribution error, also known as correspondence bias or attribution effect, is a cognitive attribution bias in which observers underemphasize situational and environmental factors for the behavior of an actor while overemphasizing dispositional or personality factors. In other words, observers tend to overattribute the behaviors of others to their personality (e.g., he is late because he's selfish) and underattribute them to the situation or context (e.g., he is late because he got stuck in traffic). Although personality traits and predispositions are considered to be observable facts in psychology, the fundamental attribution error is an error because it misinterprets their effects.\n"]},
{"name": "Defensive attribution hypothesis", "link": "https:\/\/en.wikipedia.org\/wiki\/Defensive_attribution_hypothesis", "ptags": ["The defensive attribution hypothesis (or bias, theory, or simply defensive attribution) is a social psychological term where an observer attributes the causes for a mishap to minimize their fear of being a victim or a cause in a similar situation. The attributions of blame are negatively correlated to similarities between the observer and the people involved in the mishap, i.e. more responsibility is attributed to the people involved who are dissimilar to the observer. Assigning responsibility allows the observer to believe that the mishap was controllable and thus preventable.[1]\n", "A defensive attribution may also be used to protect the person's self-esteem if, despite everything, the mishap does occur, because blame can be assigned to the \"other\" (person or situation).[2] The use of defensive attributions is considered a cognitive bias because an individual will change their beliefs about a situation based upon their motivations or desires rather than the factual characteristics of the situation.[2]:\u200a112\u200a\n"]},
{"name": "Trait ascription bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Trait_ascription_bias", "ptags": ["Trait ascription bias is the tendency for people to view themselves as relatively variable in terms of personality, behavior and mood while viewing others as much more predictable in their personal traits across different situations.[1] More specifically, it is a tendency to describe one's own behaviour in terms of situational factors while preferring to describe another's behaviour by ascribing fixed dispositions to their personality. This may occur because peoples' own internal states are more readily observable and available to them than those of others.\n", "This attributional bias intuitively plays a role in the formation and maintenance of stereotypes and prejudice, combined with the negativity effect. However, trait ascription and trait-based models of personality remain contentious in modern psychology and social science research. Trait ascription bias refers to the situational and dispositional evaluation and description of personality traits on a personal level. A similar bias on the group level is called the outgroup homogeneity bias.\n"]},
{"name": "Effort justification", "link": "https:\/\/en.wikipedia.org\/wiki\/Effort_justification", "ptags": ["Effort justification is an idea and paradigm in social psychology stemming from Leon Festinger's theory of cognitive dissonance.[1] Effort justification is a person's tendency to attribute the value of an outcome they put effort into achieving as greater than the objective value of the outcome.\n"]},
{"name": "Risk compensation", "link": "https:\/\/en.wikipedia.org\/wiki\/Risk_compensation", "ptags": ["Risk compensation  is a theory which suggests that people typically adjust their behavior in response to perceived levels of risk, becoming more careful where they sense greater risk and less careful if they feel more protected.[2] Although usually small in comparison to the fundamental benefits of safety interventions, it may result in a lower net benefit than expected or even higher risks.[3][n 1]\n", "By way of example, it has been observed that motorists drove closer to the vehicle in front when the vehicles were fitted with anti-lock brakes. There is also evidence that the risk compensation phenomenon could explain the failure of condom distribution programs to reverse HIV prevalence and that condoms may foster disinhibition, with people engaging in risky sex both with and without condoms.\n", "By contrast, shared space is an urban street design method which consciously aims to increase the level of perceived risk and uncertainty, thereby slowing traffic and reducing the number and seriousness of injuries.\n"]},
{"name": "Peltzman effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Risk_compensation#Peltzman_effect", "ptags": ["Risk compensation  is a theory which suggests that people typically adjust their behavior in response to perceived levels of risk, becoming more careful where they sense greater risk and less careful if they feel more protected.[2] Although usually small in comparison to the fundamental benefits of safety interventions, it may result in a lower net benefit than expected or even higher risks.[3][n 1]\n", "By way of example, it has been observed that motorists drove closer to the vehicle in front when the vehicles were fitted with anti-lock brakes. There is also evidence that the risk compensation phenomenon could explain the failure of condom distribution programs to reverse HIV prevalence and that condoms may foster disinhibition, with people engaging in risky sex both with and without condoms.\n", "By contrast, shared space is an urban street design method which consciously aims to increase the level of perceived risk and uncertainty, thereby slowing traffic and reducing the number and seriousness of injuries.\n"]}
   
        ]
       },
   
       {
        "name": "To stay focused, we favor the immediate, relatable thing in front of us",
        "children": [
   
{"name": "Hyperbolic discounting", "link": "https:\/\/en.wikipedia.org\/wiki\/Hyperbolic_discounting", "ptags": ["In economics, hyperbolic discounting is a time-inconsistent model of delay discounting. It is one of the cornerstones of behavioral economics[1][2] and its brain-basis is actively being studied by neuroeconomics researchers.[3]\n", "According to the discounted utility approach, intertemporal choices are no different from other choices, except that some consequences are delayed and hence must be anticipated and discounted (i.e., reweighted to take into account the delay).\n", "Given two similar rewards, humans show a preference for one that arrives in a more prompt timeframe. Humans are said to discount the value of the later reward, by a factor that increases with the length of the delay. In the financial world, this process is normally modeled in the form of exponential discounting, a time-consistent model of discounting. Many psychological studies have since demonstrated deviations in instinctive preference from the constant discount rate assumed in exponential discounting.[4] Hyperbolic discounting is an alternative mathematical model that agrees more closely with these findings.[5]\n", "According to hyperbolic discounting, valuations fall relatively rapidly for earlier delay periods (as in, from now to one week), but then fall more slowly for longer delay periods (for instance, more than a few days). For example, in an early study subjects said they would be indifferent between receiving $15 immediately or $30 after 3 months, $60 after 1 year, or $100 after 3 years. These indifferences reflect annual discount rates that declined from 277% to 139% to 63% as delays got longer.[6]  This contrasts with exponential discounting, in which valuation falls by a constant factor per unit delay and the discount rate stays the same.\n", "The standard experiment used to reveal a test subject's hyperbolic discounting curve is to compare short-term preferences with long-term preferences. For instance: \"Would you prefer a dollar today or three dollars tomorrow?\" or \"Would you prefer a dollar in one year or three dollars in one year and one day?\" It has been claimed that a significant fraction of subjects will take the lesser amount today, but will gladly wait one extra day in a year in order to receive the higher amount instead.[6]  Individuals with such preferences are described as \"present-biased\".\n", "The most important consequence of hyperbolic discounting is that it creates temporary preferences for small rewards that occur sooner over larger, later ones. Individuals using hyperbolic discounting reveal a strong tendency to make choices that are inconsistent over time \u2013 they make choices today that their future self would prefer not to have made, despite knowing the same information. This dynamic inconsistency happens because hyperbolas distort the relative value of options with a fixed difference in delays in proportion to how far the choice-maker is from those options.[7]\n"]},
{"name": "Appeal to novelty", "link": "https:\/\/en.wikipedia.org\/wiki\/Appeal_to_novelty", "ptags": ["The appeal to novelty (also called appeal to modernity or argumentum ad novitatem) is a fallacy in which one prematurely claims that an idea or proposal is correct or superior, exclusively because it is new and modern. In a controversy between status quo and new inventions, an appeal to novelty argument is not in itself a valid argument. The fallacy may take two forms: overestimating the new and modern, prematurely and without investigation assuming it to be best-case, or underestimating status quo, prematurely and without investigation assuming it to be worst-case.\n", "Investigation may prove these claims to be true, but it is a fallacy to prematurely conclude this only from the general claim that all novelty is good.\n", "Chronological snobbery is a form of appeal to novelty, in which one argues that the only relevant knowledge and practices are those established in the last decades. The opposite of an appeal to novelty is an appeal to tradition, in which one argues that the \"old ways\" are always superior to new ideas.\n", "Appeals to novelty are often successful in a modern world where everyone is eager to be on the \"cutting edge\" of technology. The dot-com bubble of the early 2000s could easily be interpreted as a sign of the dangers of na\u00efvely embracing new ideas without first viewing them with a critical eye. Also, advertisers frequently extoll the newness of their products as a reason to buy.  Conversely, this is satirised by skeptics as bleeding edge technology, which may itself be an example of an appeal to tradition.\n"]},
{"name": "Identifiable victim effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Identifiable_victim_effect", "ptags": ["The identifiable victim effect is the tendency of individuals to offer greater aid when a specific, identifiable person (\"victim\") is observed under hardship, as compared to a large, vaguely defined group with the same need.[1]\n", "The identifiable victim effect has two components. People are more inclined to help an identified victim than an unidentified one, and people are more inclined to help a single identified victim than a group of identified victims. Although helping an identified victim may be commendable, the identifiable victim effect is considered a cognitive bias. From a consequentialist point of view, the cognitive error is the failure to offer N times as much help to N unidentified victims.\n", "The identifiable victim effect has a mirror image that is sometimes called the identifiable perpetrator effect. Research has shown that individuals are more inclined to mete out punishment, even at their own expense, when they are punishing a specific, identified perpetrator.[2]\n", "The conceptualization of the identifiable victim effect as it is known today is commonly attributed to American economist Thomas Schelling. He wrote that harm to a particular person invokes \u201canxiety and sentiment, guilt and awe, responsibility and religion, [but]\u2026most of this awesomeness disappears when we deal with statistical death\u201d.[3]\n", "Historical figures from Joseph Stalin to Mother Teresa are credited with statements that epitomize the identifiable victim effect. The remark \"One death is a tragedy; a million deaths is a statistic\" is widely, although probably incorrectly, attributed to Stalin.[4] The remark \"If I look at the mass I will never act. If I look at the one, I will,\" is attributed to Mother Teresa.[5]\n"]}
   
        ]
       },
   
       {
        "name": "To get anything done, we tend to complete things we've invested time & energy in.",
        "children": [
   
{"name": "Sunk cost fallacy", "link": "https:\/\/en.wikipedia.org\/wiki\/Sunk_cost#Loss_aversion_and_the_sunk_cost_fallacy", "ptags": ["In economics and business decision-making, a sunk cost (also known as retrospective cost) is a cost that has already been incurred and cannot be recovered.[1][2] Sunk costs are contrasted with prospective costs, which are future costs that may be avoided if action is taken.[3] In other words, a sunk cost is a sum paid in the past that is no longer relevant to decisions about the future. Even though economists argue that sunk costs are no longer relevant to future rational decision-making, people in everyday life often take previous expenditures in situations, such as repairing a car or house, into their future decisions regarding those properties.\n"]},
{"name": "Escalation of commitment", "link": "https:\/\/en.wikipedia.org\/wiki\/Escalation_of_commitment", "ptags": ["\n", "\nEscalation of commitment is a human behavior pattern in which an individual or group facing increasingly negative outcomes from a decision, action, or investment nevertheless continue the behavior instead of altering course. The actor maintains behaviors that are irrational, but align with previous decisions and actions.[1]\n", "Economists and behavioral scientists use a related term, sunk-cost fallacy, to describe the justification of increased investment of money or effort in a decision, based on the cumulative prior investment (\"sunk cost\") despite new evidence suggesting that the future cost of continuing the behavior outweighs the expected benefit.\n", "In sociology, irrational escalation of commitment or commitment bias describe similar behaviors. The phenomenon and the sentiment underlying them are reflected in such proverbial images as \"throwing good money after bad\", or \"In for a penny, in for a pound\", or \"It's never the wrong time to make the right decision\", or \"If you find yourself in a hole, stop digging.\"\n"]},
{"name": "Escalation of commitment", "link": "https:\/\/en.wikipedia.org\/wiki\/Escalation_of_commitment", "ptags": ["\n", "\nEscalation of commitment is a human behavior pattern in which an individual or group facing increasingly negative outcomes from a decision, action, or investment nevertheless continue the behavior instead of altering course. The actor maintains behaviors that are irrational, but align with previous decisions and actions.[1]\n", "Economists and behavioral scientists use a related term, sunk-cost fallacy, to describe the justification of increased investment of money or effort in a decision, based on the cumulative prior investment (\"sunk cost\") despite new evidence suggesting that the future cost of continuing the behavior outweighs the expected benefit.\n", "In sociology, irrational escalation of commitment or commitment bias describe similar behaviors. The phenomenon and the sentiment underlying them are reflected in such proverbial images as \"throwing good money after bad\", or \"In for a penny, in for a pound\", or \"It's never the wrong time to make the right decision\", or \"If you find yourself in a hole, stop digging.\"\n"]},
{"name": "Generation effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Generation_effect", "ptags": ["The generation effect is a phenomenon whereby information is better remembered if it is generated from one's own mind rather than simply read.[1] Researchers have struggled to account for why the generated information is better recalled than read information, but no single explanation has been sufficient to explain everything.\n"]},
{"name": "Loss aversion", "link": "https:\/\/en.wikipedia.org\/wiki\/Loss_aversion", "ptags": ["\n", "Loss aversion is a psychological and economic concept,[1] which refers to how outcomes are interpreted as gains and losses where losses are subject to more sensitivity in people's responses compared to equivalent gains acquired.[2] Kahneman and Tversky (1992) suggested that losses can be twice as powerful psychologically as gains.[3]\n", "When defined in terms of the utility function shape as in the cumulative prospect theory (CPT), losses have a steeper utility than gains, thus being more \"painful\" than the satisfaction from a comparable gain,[4] as shown in Figure 1. Loss aversion was first proposed by Amos Tversky and Daniel Kahneman as an important framework for prospect theory \u2013 an analysis of decision under risk.[5] Finance and insurance are the sub fields of economics with the most active applications.[6]\n"]},
{"name": "IKEA effect", "link": "https:\/\/en.wikipedia.org\/wiki\/IKEA_effect", "ptags": ["The IKEA effect is a cognitive bias in which consumers place a disproportionately high value on products they partially created. The name refers to Swedish manufacturer and furniture retailer IKEA, which sells many items of furniture that require assembly.\n", "A 2011 study found that subjects were willing to pay 63% more for furniture they had assembled themselves than for equivalent pre-assembled items.[1]\n"]},
{"name": "Unit bias", "link": "https:\/\/en.wikipedia.org\/wiki\/List_of_cognitive_biases", "ptags": ["Cognitive biases are systematic patterns of deviation from norm and\/or rationality in judgment. They are often studied in psychology, sociology and behavioral economics.[1]\n", "Although the reality of most of these biases is confirmed by reproducible research,[2][3] there are often controversies about how to classify these biases or how to explain them.[4] Several theoretical causes are known for some cognitive biases, which provides a classification of biases by their common generative mechanism (such as noisy information-processing[5]). Gerd Gigerenzer has criticized the framing of cognitive biases as errors in judgment, and favors interpreting them as arising from rational deviations from logical thought.[6]\n", "Explanations include information-processing rules (i.e., mental shortcuts), called heuristics, that the brain uses to produce decisions or judgments. Biases have a variety of forms and appear as cognitive (\"cold\") bias, such as mental noise,[5] or motivational (\"hot\") bias, such as when beliefs are distorted by wishful thinking. Both effects can be present at the same time.[7][8]\n", "There are also controversies over some of these biases as to whether they count as useless or irrational, or whether they result in useful attitudes or behavior. For example, when getting to know others, people tend to ask leading questions which seem biased towards confirming their assumptions about the person. However, this kind of confirmation bias has also been argued to be an example of social skill; a way to establish a connection with the other person.[9]\n", "Although this research overwhelmingly involves human subjects, some findings that demonstrate bias have been found in non-human animals as well. For example, loss aversion has been shown in monkeys and hyperbolic discounting has been observed in rats, pigeons, and monkeys.[10]\n"]},
{"name": "Zero-risk bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Zero-risk_bias", "ptags": ["Zero-risk bias is a tendency to prefer the complete elimination of risk in a sub-part over alternatives with greater overall risk reduction.[1] It often manifests in cases where decision makers address problems concerning health, safety, and the environment.[2] Its effect on decision making has been observed in surveys presenting hypothetical scenarios.[3]\n"]},
{"name": "Disposition effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Disposition_effect", "ptags": ["The disposition effect is an anomaly discovered in behavioral finance. It relates to the tendency of investors to sell assets that have increased in value, while keeping assets that have dropped in value.[1]\n", "Hersh Shefrin and Meir Statman identified and named the effect in their 1985 paper, which found that people dislike losing significantly more than they enjoy winning. The disposition effect has been described as one of the foremost vigorous actualities around individual investors because investors will hold stocks that have lost value yet sell stocks that have gained value.\"[2]\n", "In 1979, Daniel Kahneman and Amos Tversky traced the cause of the disposition effect to the so-called \"prospect theory\".[3] The prospect theory proposes that when an individual is presented with two equal choices, one having possible gains and the other with possible losses, the individual is more likely to opt for the former choice even though both would yield the same economic result.\n", "The disposition effect can be minimized by a mental approach called \"hedonic framing\".\n"]},
{"name": "Pseudocertainty effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Pseudocertainty_effect", "ptags": ["In prospect theory, the pseudocertainty effect is the tendency for people to perceive an outcome as certain while it is actually uncertain in multi-stage decision making. The evaluation of the certainty of the outcome in a previous stage of decisions is disregarded when selecting an option in subsequent stages. Not to be confused with certainty effect, the pseudocertainty effect was discovered from an attempt at providing a normative use of decision theory for the certainty effect by relaxing the cancellation rule.[1]\n"]},
{"name": "Processing difficulty effect", "link": "https:\/\/en.wikipedia.org\/wiki\/List_of_cognitive_biases#Processing_difficulty_effect", "ptags": ["Cognitive biases are systematic patterns of deviation from norm and\/or rationality in judgment. They are often studied in psychology, sociology and behavioral economics.[1]\n", "Although the reality of most of these biases is confirmed by reproducible research,[2][3] there are often controversies about how to classify these biases or how to explain them.[4] Several theoretical causes are known for some cognitive biases, which provides a classification of biases by their common generative mechanism (such as noisy information-processing[5]). Gerd Gigerenzer has criticized the framing of cognitive biases as errors in judgment, and favors interpreting them as arising from rational deviations from logical thought.[6]\n", "Explanations include information-processing rules (i.e., mental shortcuts), called heuristics, that the brain uses to produce decisions or judgments. Biases have a variety of forms and appear as cognitive (\"cold\") bias, such as mental noise,[5] or motivational (\"hot\") bias, such as when beliefs are distorted by wishful thinking. Both effects can be present at the same time.[7][8]\n", "There are also controversies over some of these biases as to whether they count as useless or irrational, or whether they result in useful attitudes or behavior. For example, when getting to know others, people tend to ask leading questions which seem biased towards confirming their assumptions about the person. However, this kind of confirmation bias has also been argued to be an example of social skill; a way to establish a connection with the other person.[9]\n", "Although this research overwhelmingly involves human subjects, some findings that demonstrate bias have been found in non-human animals as well. For example, loss aversion has been shown in monkeys and hyperbolic discounting has been observed in rats, pigeons, and monkeys.[10]\n"]},
{"name": "Endowment effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Endowment_effect", "ptags": ["In psychology and behavioral economics, the endowment effect (also known as divestiture aversion) is the finding that people are more likely to retain an object they own than acquire that same object when they do not own it.[1][2][3][4] The endowment theory can be defined as \"an application of prospect theory positing that loss aversion associated with ownership explains observed exchange asymmetries.\"[5]\n", "This is typically illustrated in two ways.[2] In a valuation paradigm, people's maximum willingness to pay (WTP) to acquire an object is typically lower than the least amount they are willing to accept (WTA) to give up that same object when they own it\u2014even when there is no cause for attachment, or even if the item was only obtained minutes ago.[4] In an exchange paradigm, people given a good are reluctant to trade it for another good of similar value. For example, participants first given a pen of equal expected value to that of a coffee mug were generally unwilling to trade, whilst participants first given the coffee mug were also unwilling to trade it for the pen.[6]\n", "A more controversial third paradigm used to elicit the endowment effect is the mere ownership paradigm, primarily used in experiments in psychology, marketing, and organizational behavior. In this paradigm, people who are randomly assigned to receive a good (\"owners\") evaluate it more positively than people who are not randomly assigned to receive the good (\"controls\").[7][2] The distinction between this paradigm and the first two is that it is not incentive-compatible. In other words, participants are not explicitly incentivized to reveal the extent to which they truly like or value the good.\n", "The endowment effect can be equated to the behavioural model willingness to accept or pay (WTAP), a formula sometimes used to find out how much a consumer or person is willing to put up with or lose for different outcomes. However, this model has come under recent criticism as potentially inaccurate.[5][8]\n"]},
{"name": "Backfire effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Confirmation_bias#backfire_effect", "ptags": ["\nConfirmation bias (also confirmatory bias, myside bias,[a] or congeniality bias[2]) is the tendency to search for, interpret, favor, and recall information in a way that confirms or supports one's prior beliefs or values.[3] People display this bias when they select information that supports their views, ignoring contrary information, or when they interpret ambiguous evidence as supporting their existing attitudes. The effect is strongest for desired outcomes, for emotionally charged issues, and for deeply entrenched beliefs. Confirmation bias is insuperable for most people, but they can manage it, for example, by education and training in critical thinking skills.\n", "Biased search for information, biased interpretation of this information, and biased memory recall, have been invoked to explain four specific effects:\n", "A series of psychological experiments in the 1960s suggested that people are biased toward confirming their existing beliefs. Later work re-interpreted these results as a tendency to test ideas in a one-sided way, focusing on one possibility and ignoring alternatives. Explanations for the observed biases include wishful thinking and the limited human capacity to process information. Another proposal is that people show confirmation bias because they are pragmatically assessing the costs of being wrong, rather than investigating in a neutral, scientific way.\n", "Flawed decisions due to confirmation bias have been found in a wide range of political, organizational, financial and scientific contexts. These biases contribute to overconfidence in personal beliefs and can maintain or strengthen beliefs in the face of contrary evidence. For example, confirmation bias produces systematic errors in scientific research based on inductive reasoning (the gradual accumulation of supportive evidence). Similarly, a police detective may identify a suspect early in an investigation, but then may only seek confirming rather than disconfirming evidence. A medical practitioner may prematurely focus on a particular disorder early in a diagnostic session, and then seek only confirming evidence. In social media, confirmation bias is amplified by the use of filter bubbles, or \"algorithmic editing\", which display to individuals only information they are likely to agree with, while excluding opposing views.\n"]}
   
        ]
       },
   
       {
        "name": "To avoid mistakes, we tend to preserve our autonomy and group status, and avoid irreversible decisions.",
        "children": [
   
{"name": "System justification", "link": "https:\/\/en.wikipedia.org\/wiki\/System_justification", "ptags": ["System justification theory is a theory within social psychology that system-justifying beliefs serve a psychologically palliative function. It proposes that people have several underlying needs, which vary from individual to individual, that can be satisfied by the defense and justification of the status quo, even when the system may be disadvantageous to certain people. People have epistemic, existential, and relational needs that are met by and manifest as ideological support for the prevailing structure of social, economic, and political norms. Need for order and stability, and thus resistance to change or alternatives, for example, can be a motivator for individuals to see the status quo as good, legitimate, and even desirable.\n", "According to system justification theory, people desire not only to hold favorable attitudes about themselves (ego-justification) and the groups to which they belong (group-justification), but also to hold positive attitudes about the overarching social structure in which they are entwined and find themselves obligated to (system-justification). This system-justifying motive sometimes produces the phenomenon known as out-group favoritism, an acceptance of inferiority among low-status groups and a positive image of relatively higher status groups. Thus, the notion that individuals are simultaneously supporters and victims of the system-instilled norms is a central idea in system justification theory. Additionally, the passive ease of supporting the current structure, when compared to the potential price (material, social, psychological) of acting out against the status quo, leads to a shared environment in which the existing social, economic, and political arrangements tend to be preferred. Alternatives to the status quo tend to be disparaged, and inequality tends to perpetuate.[1][2]\n"]},
{"name": "Reverse psychology", "link": "https:\/\/en.wikipedia.org\/wiki\/Reverse_psychology", "ptags": ["Reverse psychology is a technique involving the assertion of a belief or behavior that is opposite to the one desired, with the expectation that this approach will encourage the subject of the persuasion to do what is actually desired. This technique relies on the psychological phenomenon of reactance, in which a person has a negative emotional reaction to being persuaded, and thus chooses the option which is being advocated against.[1] This may work especially well on a person who is resistant by nature, while direct requests work best for people who are compliant.[2] The one being manipulated is usually unaware of what is really going on.[3]\n"]},
{"name": "Reactance", "link": "https:\/\/en.wikipedia.org\/wiki\/Reactance_(psychology)", "ptags": ["In psychology, reactance is an unpleasant motivational reaction to offers, persons, rules, or regulations that threaten or eliminate specific behavioral freedoms. Reactance occurs when an individual feels that an agent is attempting to limit one's choice of response and\/or range of alternatives.\n", "Reactance can occur when someone is heavily pressured into accepting a certain view or attitude. Reactance can encourage an individual to adopt or strengthen a view or attitude which is indeed contrary to that which was intended\u2009\u2014\u2009which is to say, to a response of noncompliance\u2009\u2014\u2009and can also increase resistance to persuasion. Some individuals might employ reverse psychology in a bid to exploit reactance for their benefit, in an attempt to influence someone to choose the opposite of what is being requested. Reactance can occur when an individual senses that someone is trying to compel them to do something; often the individual will offer resistance and attempt to extricate themselves from the situation.\n", "Some individuals are naturally high in reactance, a personality characteristic called trait reactance.[1]\n"]},
{"name": "Decoy effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Decoy_effect", "ptags": ["In marketing, the decoy effect (or attraction effect or asymmetric dominance effect) is the phenomenon whereby consumers will tend to have a specific change in preference between two options when also presented with a third option that is asymmetrically dominated.[1] An option is asymmetrically dominated when it is inferior in all respects to one option; but, in comparison to the other option, it is inferior in some respects and superior in others. In other words, in terms of specific attributes determining preferences, it is completely dominated by (i.e., inferior to) one option and only partially dominated by the other. When the asymmetrically dominated option is present, a higher percentage of consumers will prefer the dominating option than when the asymmetrically dominated option is absent. The asymmetrically dominated option is therefore a decoy serving to increase preference for the dominating option. The decoy effect is also an example of the violation of the independence of irrelevant alternatives axiom of decision theory.  More simply, when deciding between two options, an unattractive third option can change the perceived preference between the other two.[2]\n", "The decoy effect is considered particularly important in choice theory because it is a violation of the assumption of \"regularity\" present in all axiomatic choice models, for example in a Luce model of choice.[3]  Regularity means that it should not be possible for the market share of any alternative to increase when another alternative is added to the choice set. The new alternative should reduce, or at best leave unchanged, the choice share of existing alternatives. Regularity is violated in the example shown below where a new alternative C not only changes the relative shares of A and B but actually increases the share of A in absolute terms.  Similarly, the introduction of a new alternative D increases the share of B in absolute terms.\n"]},
{"name": "Social comparison bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Social_comparison_bias", "ptags": ["Social comparison bias is the tendency to have feelings of dislike and competitiveness with someone seen as physically, socially, or mentally better than oneself. Social comparison bias or social comparison theory is the idea that individuals determine their own worth based on how they compare to others. The theory was developed in 1954 by psychologist Leon Festinger. This can be compared to social comparison, which is believed to be central to achievement motivation, feelings of injustice, depression, jealousy, and people's willingness to remain in relationships or jobs.[1][2] The basis of the theory is that people are believed to compete for the best outcome in relation to their peers. For example, one might make a comparison between the low-end department stores they go to frequently and the designer stores of their peers. Such comparisons may evoke feelings of resentment, anger , and envy with their peers. This bias revolves mostly around wealth and social status; it is unconscious and people who make these are largely unaware of them.[3]  In most cases, people try to compare themselves to those in their peer group or with whom they are similar.[4]\n"]},
{"name": "Status quo bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Status_quo_bias", "ptags": ["Status quo bias is an emotional bias; a preference for the maintenance of one's current or previous state of affairs, or a preference to not undertake any action to change this current or previous state.[1] The current baseline (or status quo) is taken as a reference point, and any change from that baseline is perceived as a loss or gain. Corresponding to different alternatives, this current baseline or default option is perceived and evaluated by individuals as a positive.[2]\n", "Status quo bias should be distinguished from a rational preference for the status quo ante, as when the current state of affairs is objectively superior to the available alternatives, or when imperfect information is a significant problem. A large body of evidence, however, shows that status quo bias frequently affects human decision-making. Status quo bias should also be distinguished from psychological inertia, which refers to a lack of intervention in the current course of affairs.\n", "The bias intersects with other non-rational cognitive processes such as loss aversion, in which losses comparative to gains are weighed to a greater extent.[2] Further non-rational cognitive processes include existence bias, endowment effect, longevity, mere exposure, and regret avoidance.  Experimental evidence for the detection of status quo bias is seen through the use of the reversal test. A vast amount of experimental and field examples exist. Behaviour in regard to economics, retirement plans, health, and ethical choices show evidence of the status quo bias.\n"]}
   
        ]
       },
   
       {
        "name": "We favor options that appear simple or have more complete information over more complex, ambiguous options.",
        "children": [
   
{"name": "Ambiguity bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Ambiguity_effect", "ptags": ["The ambiguity effect is a cognitive tendency where decision making is affected by a lack of information, or \"ambiguity\".[1] The effect implies that people tend to select options for which the probability of a favorable outcome is known, over an option for which the probability of a favorable outcome is unknown. The effect was first described by Daniel Ellsberg in 1961.[2]\n"]},
{"name": "Information bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Information_bias_(psychology)", "ptags": ["Information bias is a cognitive bias to seek information when it does not affect action. An example of information bias is believing that the more information that can be acquired to make a decision, the better, even if that extra information is irrelevant for the decision.[1]\n"]},
{"name": "Belief bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Belief_bias", "ptags": ["Belief bias is the tendency to judge the strength of arguments based on the plausibility of their conclusion rather than how strongly they justify that conclusion.[1] A person is more likely to accept an argument that supports a conclusion that aligns with their values, beliefs and prior knowledge, while rejecting counter arguments to the conclusion.[2]  Belief bias is an extremely common and therefore significant form of error; we can easily be blinded by our beliefs and reach the wrong conclusion. Belief bias has been found to influence various reasoning tasks, including conditional reasoning,[3] relation reasoning[4] and transitive reasoning.[5]\n"]},
{"name": "Rhyme as reason effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Rhyme-as-reason_effect", "ptags": ["The rhyme-as-reason effect, also known as the Eaton\u2013Rosen phenomenon,[1][2][3] is a cognitive bias where sayings or aphorisms are perceived as more accurate or truthful when they rhyme.\n", "In experiments, participants evaluated variations of sayings that either rhymed or did not rhyme. Those that rhymed were consistently judged as more truthful, even when the meaning was controlled for. For instance, the rhyming saying \"What sobriety conceals, alcohol reveals\" was rated as more accurate on average than its non-rhyming counterpart, \"What sobriety conceals, alcohol unmasks,\" across different groups of subjects (each group assessed the accuracy of only one version of the statement).[4]\n", "This effect may be explained by the Keats heuristic, which suggests that people assess a statement's truth based on its aesthetic qualities.[5] Another explanation is the fluency heuristic, which posits that statements are preferred due to their ease of cognitive processing.[6]\n"]},
{"name": "Law of Triviality", "link": "https:\/\/en.wikipedia.org\/wiki\/Law_of_triviality", "ptags": ["\n", "The law of triviality is C. Northcote Parkinson's 1957 argument that people within an organization commonly give disproportionate weight to trivial issues.[1] Parkinson provides the example of a fictional committee whose job was to approve the plans for a nuclear power plant spending the majority of its time on discussions about relatively minor but easy-to-grasp issues, such as what materials to use for the staff bicycle shed, while neglecting the proposed design of the plant itself, which is far more important and a far more difficult and complex task.\n", "The law has been applied to software development and other activities.[2] The terms bicycle-shed effect, bike-shed effect, and bike-shedding were coined based on Parkinson's example; it was popularised in the Berkeley Software Distribution community by the Danish software developer Poul-Henning Kamp in 1999[3] and, due to that, has since become popular within the field of software development generally.\n"]},
{"name": "Delmore effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Law_of_triviality", "ptags": ["\n", "The law of triviality is C. Northcote Parkinson's 1957 argument that people within an organization commonly give disproportionate weight to trivial issues.[1] Parkinson provides the example of a fictional committee whose job was to approve the plans for a nuclear power plant spending the majority of its time on discussions about relatively minor but easy-to-grasp issues, such as what materials to use for the staff bicycle shed, while neglecting the proposed design of the plant itself, which is far more important and a far more difficult and complex task.\n", "The law has been applied to software development and other activities.[2] The terms bicycle-shed effect, bike-shed effect, and bike-shedding were coined based on Parkinson's example; it was popularised in the Berkeley Software Distribution community by the Danish software developer Poul-Henning Kamp in 1999[3] and, due to that, has since become popular within the field of software development generally.\n"]},
{"name": "Conjunction fallacy", "link": "https:\/\/en.wikipedia.org\/wiki\/Conjunction_fallacy", "ptags": ["The conjunction fallacy (also known as the Linda problem) is an inference that a conjoint set of two or more specific conclusions is likelier than any single member of that same set, in violation of the laws of probability. It is a type of formal fallacy.\n"]},
{"name": "Occam's razor", "link": "https:\/\/en.wikipedia.org\/wiki\/Occam's_razor", "ptags": ["\nIn philosophy, Occam's razor (also spelled Ockham's razor or Ocham's razor; Latin: novacula Occami) is the problem-solving principle that recommends searching for explanations constructed with the smallest possible set of elements. It is also known as the principle of parsimony or the law of parsimony (Latin: lex parsimoniae). Attributed to William of Ockham, a 14th-century English philosopher and theologian, it is frequently cited as Entia non sunt multiplicanda praeter necessitatem, which translates as \"Entities must not be multiplied beyond necessity\",[1][2] although Occam never used these exact words. Popularly, the principle is sometimes paraphrased as \"The simplest explanation is usually the best one.\"[3]\n", "This philosophical razor advocates that when presented with competing hypotheses about the same prediction and both theories have equal explanatory power one should prefer the hypothesis that requires the fewest assumptions[4] and that this is not meant to be a way of choosing between hypotheses that make different predictions. Similarly, in science, Occam's razor is used as an abductive heuristic in the development of theoretical models rather than as a rigorous arbiter between candidate models.[5][6]\n"]},
{"name": "Less-is-better effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Less-is-better_effect", "ptags": ["The less-is-better effect is a type of preference reversal that occurs when the lesser or smaller alternative of a proposition is preferred when evaluated separately, but not evaluated together. The term was first proposed by Christopher Hsee.[1]\n"]}
   
        ]
       }
   
      ]
     },
   
   
     {
      "name": "4. What Should We Remember?",
      "children": [
   
       {
        "name": "We edit and reinforce some memories after the fact",
        "children": [
   
{"name": "Misattribution of memory", "link": "https:\/\/en.wikipedia.org\/wiki\/Misattribution_of_memory", "ptags": ["In psychology, the misattribution of memory or source misattribution is the misidentification of the origin of a memory by the person making the memory recall. Misattribution is likely to occur when individuals are unable to monitor and control the influence of their attitudes, toward their judgments, at the time of retrieval.[1] Misattribution is divided into three components: cryptomnesia, false memories, and source confusion. It was originally noted as one of Daniel Schacter's seven sins of memory.[2]\n"]},
{"name": "Source confusion", "link": "https:\/\/en.wikipedia.org\/wiki\/Misattribution_of_memory#Source_confusion", "ptags": ["In psychology, the misattribution of memory or source misattribution is the misidentification of the origin of a memory by the person making the memory recall. Misattribution is likely to occur when individuals are unable to monitor and control the influence of their attitudes, toward their judgments, at the time of retrieval.[1] Misattribution is divided into three components: cryptomnesia, false memories, and source confusion. It was originally noted as one of Daniel Schacter's seven sins of memory.[2]\n"]},
{"name": "Cryptomnesia", "link": "https:\/\/en.wikipedia.org\/wiki\/Cryptomnesia", "ptags": ["Cryptomnesia occurs when a forgotten memory returns without its being recognized as such by the subject, who believes it is something new and original. It is a memory bias whereby a person may falsely recall generating a thought, an idea, a tune, a name, or a joke;[1] they are not deliberately engaging in plagiarism, but are experiencing a memory as if it were a new inspiration.\n"]},
{"name": "Suggestibility", "link": "https:\/\/en.wikipedia.org\/wiki\/Suggestibility", "ptags": ["\nSuggestibility is the quality of being inclined to accept and act on the suggestions of others. One may fill in gaps in certain memories with false information given by another when recalling a scenario or moment. Suggestibility uses cues to distort recollection: when the subject has been persistently told something about a past event, his or her memory of the event conforms to the repeated message.[1]\n", "A person experiencing intense emotions tends to be more receptive to ideas and therefore more suggestible. Generally, suggestibility decreases as age increases. However, psychologists have found that individual levels of self-esteem and assertiveness can make some people more suggestible than others; this finding led to the concept of a spectrum of suggestibility.[2]\n"]},
{"name": "Suggestibility", "link": "https:\/\/en.wikipedia.org\/wiki\/Suggestibility", "ptags": ["\nSuggestibility is the quality of being inclined to accept and act on the suggestions of others. One may fill in gaps in certain memories with false information given by another when recalling a scenario or moment. Suggestibility uses cues to distort recollection: when the subject has been persistently told something about a past event, his or her memory of the event conforms to the repeated message.[1]\n", "A person experiencing intense emotions tends to be more receptive to ideas and therefore more suggestible. Generally, suggestibility decreases as age increases. However, psychologists have found that individual levels of self-esteem and assertiveness can make some people more suggestible than others; this finding led to the concept of a spectrum of suggestibility.[2]\n"]},
{"name": "Spacing effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Spacing_effect", "ptags": ["\n", "The spacing effect demonstrates that learning is more effective when study sessions are spaced out.  This effect shows that more information is encoded into long-term memory by spaced study sessions, also known as spaced repetition or spaced presentation, than by massed presentation (\"cramming\").\n", "The phenomenon was first identified by Hermann Ebbinghaus, and his detailed study of it was published in the 1885 book \u00dcber das Ged\u00e4chtnis. Untersuchungen zur experimentellen Psychologie (Memory: A Contribution to Experimental Psychology), which suggests that active recall with increasing time intervals reduces the probability of forgetting information. This robust finding has been supported by studies of many explicit memory tasks such as free recall, recognition, cued-recall, and frequency estimation (for reviews see Crowder 1976; Greene, 1989).\n", "Researchers have offered several possible explanations of the spacing effect, and much research has been conducted that supports its impact on recall.  In spite of these findings, the robustness of this phenomenon and its resistance to experimental manipulation have made empirical testing of its parameters difficult.\n", "While many others have contributed important research regarding the spacing effect, Robert Bjork and his associates in the Bjork Learning and Forgetting Lab and Cogfog group at UCLA have performed much research into various aspects of this phenomenon as well as into its practical application for education.[1]\n"]}
   
        ]
       },
   
       {
        "name": "We discard specifics to form generalities",
        "children": [
   
{"name": "Implicit stereotypes", "link": "https:\/\/en.wikipedia.org\/wiki\/Implicit_stereotype", "ptags": ["An implicit bias or implicit stereotype is the pre-reflective attribution of particular qualities by an individual to a member of some social out group.[1]  Recent studies have determined that \"implicit bias\" towards those of the opposite gender may be even more influential than racial implicit bias.[2]\n", "Implicit stereotypes are thought to be shaped by experience and based on learned associations between particular qualities and social categories, including race and\/or gender.[3] Individuals' perceptions and behaviors can be influenced by the implicit stereotypes they hold, even if they are sometimes unaware they hold such stereotypes.[4] Implicit bias is an aspect of implicit social cognition: the phenomenon that perceptions, attitudes, and stereotypes can operate prior to conscious intention or endorsement.[5] The existence of implicit bias is supported by a variety of scientific articles in psychological literature.[6] Implicit stereotype was first defined by psychologists Mahzarin Banaji and Anthony Greenwald in 1995.\n", "Explicit stereotypes, by contrast, are consciously endorsed, intentional, and sometimes controllable thoughts and beliefs.[7]\n", "Implicit biases, however, are thought to be the product of associations learned through past experiences.[8] Implicit biases can be activated by the environment and operate prior to a person's intentional, conscious endorsement.[1] Implicit bias can persist even when an individual rejects the bias explicitly.[1]\n"]},
{"name": "Implicit stereotypes", "link": "https:\/\/en.wikipedia.org\/wiki\/Implicit_stereotype", "ptags": ["An implicit bias or implicit stereotype is the pre-reflective attribution of particular qualities by an individual to a member of some social out group.[1]  Recent studies have determined that \"implicit bias\" towards those of the opposite gender may be even more influential than racial implicit bias.[2]\n", "Implicit stereotypes are thought to be shaped by experience and based on learned associations between particular qualities and social categories, including race and\/or gender.[3] Individuals' perceptions and behaviors can be influenced by the implicit stereotypes they hold, even if they are sometimes unaware they hold such stereotypes.[4] Implicit bias is an aspect of implicit social cognition: the phenomenon that perceptions, attitudes, and stereotypes can operate prior to conscious intention or endorsement.[5] The existence of implicit bias is supported by a variety of scientific articles in psychological literature.[6] Implicit stereotype was first defined by psychologists Mahzarin Banaji and Anthony Greenwald in 1995.\n", "Explicit stereotypes, by contrast, are consciously endorsed, intentional, and sometimes controllable thoughts and beliefs.[7]\n", "Implicit biases, however, are thought to be the product of associations learned through past experiences.[8] Implicit biases can be activated by the environment and operate prior to a person's intentional, conscious endorsement.[1] Implicit bias can persist even when an individual rejects the bias explicitly.[1]\n"]},
{"name": "Stereotypical bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Implicit_stereotype", "ptags": ["An implicit bias or implicit stereotype is the pre-reflective attribution of particular qualities by an individual to a member of some social out group.[1]  Recent studies have determined that \"implicit bias\" towards those of the opposite gender may be even more influential than racial implicit bias.[2]\n", "Implicit stereotypes are thought to be shaped by experience and based on learned associations between particular qualities and social categories, including race and\/or gender.[3] Individuals' perceptions and behaviors can be influenced by the implicit stereotypes they hold, even if they are sometimes unaware they hold such stereotypes.[4] Implicit bias is an aspect of implicit social cognition: the phenomenon that perceptions, attitudes, and stereotypes can operate prior to conscious intention or endorsement.[5] The existence of implicit bias is supported by a variety of scientific articles in psychological literature.[6] Implicit stereotype was first defined by psychologists Mahzarin Banaji and Anthony Greenwald in 1995.\n", "Explicit stereotypes, by contrast, are consciously endorsed, intentional, and sometimes controllable thoughts and beliefs.[7]\n", "Implicit biases, however, are thought to be the product of associations learned through past experiences.[8] Implicit biases can be activated by the environment and operate prior to a person's intentional, conscious endorsement.[1] Implicit bias can persist even when an individual rejects the bias explicitly.[1]\n"]},
{"name": "Prejudice", "link": "https:\/\/en.wikipedia.org\/wiki\/Prejudice", "ptags": ["Prejudice[1] can be an affective feeling towards a person based on their perceived group membership.[2] The word is often used to refer to a preconceived (usually unfavourable) evaluation or classification of another person based on that person's perceived personal characteristics, such as political affiliation, sex, gender, gender identity, beliefs, values, social class, age, disability, religion, sexuality, race, ethnicity, language, nationality, culture, complexion, beauty, height, body weight, occupation, wealth, education, criminality, sport-team affiliation, music tastes or other perceived characteristics.[3]\n", "The word \"prejudice\" can also refer to unfounded or pigeonholed beliefs[4][5] and it may apply to \"any unreasonable attitude that is unusually resistant to rational influence\".[6] Gordon Allport defined prejudice as a \"feeling, favorable or unfavorable, toward a person or thing, prior to, or not based on, actual experience\".[7] Auestad (2015) defines prejudice as characterized by \"symbolic transfer\", transfer of a value-laden meaning content onto a socially-formed category and then on to individuals who are taken to belong to that category, resistance to change, and overgeneralization.[8]\n", "The United Nations Institute on Globalization, Culture and Mobility has highlighted research considering prejudice as a  global security threat due to its use in scapegoating some populations and inciting others to commit  violent acts towards them and how this can endanger individuals, countries, and the international community.[9]\n"]},
{"name": "Negativity bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Negativity_bias", "ptags": ["The negativity bias,[1] also known as the negativity effect, is a cognitive bias that, even when positive or neutral things of equal intensity occur, things of a more negative nature (e.g. unpleasant thoughts, emotions, or social interactions; harmful\/traumatic events) have a greater effect on one's psychological state and processes than neutral or positive things.[2][3][4]  In other words, something very positive will generally have less of an impact on a person's behavior and cognition than something equally emotional but negative.  The negativity bias has been investigated within many different domains, including the formation of impressions and general evaluations; attention, learning, and memory; and decision-making and risk considerations.\n"]},
{"name": "Fading affect bias", "link": "https:\/\/en.wikipedia.org\/wiki\/Fading_affect_bias", "ptags": ["The fading affect bias, more commonly known as FAB, is a psychological phenomenon in which memories associated with negative emotions tend to be forgotten more quickly than those associated with positive emotions.[1] FAB only refers to the feelings one has associated with the memories and not the content of the memories themselves.[2] Early research studied FAB retrospectively, or through personal reflection, which brought about some criticism because retrospective analysis can be affected by subjective retrospective biases. However, new research using non-retrospective recall studies have found evidence for FAB,[1] and the phenomenon has become largely accepted.\n"]}
   
        ]
       },
   
       {
        "name": "We reduce events and lists to their key elements",
        "children": [
   
{"name": "Peak\u2013end rule", "link": "https:\/\/en.wikipedia.org\/wiki\/Peak\u2013end_rule", "ptags": ["The peak\u2013end rule is a psychological heuristic in which people judge an experience largely based on how they felt at its peak (i.e., its most intense point) and at its end, rather than based on the total sum or average of every moment of the experience. The effect occurs regardless of whether the experience is pleasant or unpleasant. To the heuristic, other information aside from that of the peak and end of the experience is not lost, but it is not used. This includes net pleasantness or unpleasantness and how long the experience lasted. The peak\u2013end rule is thereby a specific form of the more general extension neglect and duration neglect.\n"]},
{"name": "Leveling and sharpening", "link": "https:\/\/en.wikipedia.org\/wiki\/Leveling_and_sharpening", "ptags": ["Leveling and sharpening are two functions that are automatic and exist within memory. Sharpening is usually the way people remember small details in the retelling of stories they have experienced or are retelling those stories. Leveling is when people keep out parts of stories and try to tone those stories down so that some parts are excluded. Therefore, it makes it easier to fill in the memory gaps that exist.\n"]},
{"name": "Misinformation effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Misinformation_effect", "ptags": ["The misinformation effect occurs when a person's recall of episodic memories becomes less accurate because of post-event information.[1] The misinformation effect has been studied since the mid-1970s. Elizabeth Loftus is one of the most influential researchers in the field. One theory is that original information and the misleading information that was presented after the fact become blended together.[2] Another theory is that the misleading information overwrites the original information.[3] Scientists suggest that because the misleading information is the most recent, it is more easily retrieved.[4]\n", "The misinformation effect is an example of retroactive interference which occurs when information presented later interferes with the ability to retain previously encoded information. Individuals have also been shown to be susceptible to incorporating misleading information into their memory when it is presented within a question.[5] Essentially, the new information that a person receives works backward in time to distort memory of the original event.[6] One mechanism through which the misinformation effect occurs is source misattribution, in which the false information given after the event becomes incorporated into people's memory of the actual event.[7] The misinformation effect also appears to stem from memory impairment, meaning that post-event misinformation makes it harder for people to remember the event.[7] The misinformation reflects two of the cardinal sins of memory: suggestibility, the influence of others' expectations on our memory; and misattribution, information attributed to an incorrect source.\n", "Research on the misinformation effect has uncovered concerns about the permanence and reliability of memory.[8] Understanding the misinformation effect is also important given its implications for the accuracy of eyewitness testimony, as there are many chances for misinformation to be incorporated into witnesses' memories through conversations with other witnesses, police questioning, and court appearances.[9][7]\n"]},
{"name": "Serial recall effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Recall_(memory)#Serial_recall", "ptags": ["Recall in memory refers to the mental process of retrieval of information from the past. Along with encoding and storage, it is one of the three core processes of memory. There are three main types of recall: free recall, cued recall and serial recall. Psychologists test these forms of recall as a way to study the memory processes of humans[1] and animals.[2]\nTwo main theories of the process of recall are the two-stage theory and the theory of encoding specificity.\n"]},
{"name": "List-length effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Recall_(memory)#Serial_recall", "ptags": ["Recall in memory refers to the mental process of retrieval of information from the past. Along with encoding and storage, it is one of the three core processes of memory. There are three main types of recall: free recall, cued recall and serial recall. Psychologists test these forms of recall as a way to study the memory processes of humans[1] and animals.[2]\nTwo main theories of the process of recall are the two-stage theory and the theory of encoding specificity.\n"]},
{"name": "Duration neglect", "link": "https:\/\/en.wikipedia.org\/wiki\/Duration_neglect", "ptags": ["Duration neglect is the psychological observation that people's judgments of the unpleasantness of painful experiences depend very little on the duration of those experiences. Multiple experiments have found that these judgments tend to be affected by two factors: the peak (when the experience was the most painful) and how quickly the pain diminishes. If it diminishes more slowly, the experience is judged to be less painful. Hence, the term \"peak\u2013end rule\" describes this process of evaluation.[1][2]\n", "Duration neglect is a specific form of the more general extension neglect.\n"]},
{"name": "Modality effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Modality_effect", "ptags": ["The modality effect is a term used in experimental psychology, most often in the fields dealing with memory and learning, to refer to how learner performance depends on the presentation mode of studied items.\n"]},
{"name": "Memory inhibition", "link": "https:\/\/en.wikipedia.org\/wiki\/Memory_inhibition", "ptags": ["In psychology, memory inhibition is the ability not to remember irrelevant information. The scientific concept of memory inhibition should not be confused with everyday uses of the word \"inhibition\". Scientifically speaking, memory inhibition is a type of cognitive inhibition, which is the stopping or overriding of a mental process, in whole or in part, with or without intention.[1]\n", "Memory inhibition is a critical component of an effective memory system.[2] While some memories are retained for a lifetime, most memories are forgotten.[3] According to evolutionary psychologists, forgetting is adaptive because it facilitates selectivity of rapid, efficient recollection.[4] For example, a person trying to remember where they parked their car would not want to remember every place they have ever parked. In order to remember something, therefore, it is essential not only to activate the relevant information, but also to inhibit irrelevant information. \n", "There are many memory phenomena that seem to involve inhibition, although there is often debate about the distinction between interference and inhibition.\n"]},
{"name": "Primacy effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Serial-position_effect#Primacy_effect", "ptags": ["Serial-position effect is the tendency of a person to recall the first and last items in a series best, and the middle items worst.[1] The term was coined by Hermann Ebbinghaus through studies he performed on himself, and refers to the finding that recall accuracy varies as a function of an item's position within a study list.[2] When asked to recall a list of items in any order (free recall), people tend to begin recall with the end of the list, recalling those items best (the recency effect). Among earlier list items, the first few items are recalled more frequently than the middle items (the primacy effect).[3][4]\n", "One suggested reason for the primacy effect is that the initial items presented are most effectively stored in long-term memory because of the greater amount of processing devoted to them. (The first list item can be rehearsed by itself; the second must be rehearsed along with the first, the third along with the first and second, and so on.) The primacy effect is reduced when items are presented quickly and is enhanced when presented slowly (factors that reduce and enhance processing of each item and thus permanent storage). Longer presentation lists have been found to reduce the primacy effect.[4]\n", "One theorised reason for the recency effect is that these items are still present in working memory when recall is solicited. Items that benefit from neither (the middle items) are recalled most poorly. An additional explanation for the recency effect is related to temporal context: if tested immediately after rehearsal, the current temporal context can serve as a retrieval cue, which would predict more recent items to have a higher likelihood of recall than items that were studied in a different temporal context (earlier in the list).[5] The recency effect is reduced when an interfering task is given. Intervening tasks involve working memory, as the distractor activity, if exceeding 15 to 30 seconds in duration, can cancel out the recency effect.[6] Additionally, if recall comes immediately after the test, the recency effect is consistent regardless of the length of the studied list,[4] or presentation rate.[7]\n", "Amnesiacs with poor ability to form permanent long-term memories do not show a primacy effect, but do show a recency effect if recall comes immediately after study.[8] People with Alzheimer's disease exhibit a reduced primacy effect but do not produce a recency effect in recall.[9]\n"]},
{"name": "Recency effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Serial-position_effect#Recency_effect", "ptags": ["Serial-position effect is the tendency of a person to recall the first and last items in a series best, and the middle items worst.[1] The term was coined by Hermann Ebbinghaus through studies he performed on himself, and refers to the finding that recall accuracy varies as a function of an item's position within a study list.[2] When asked to recall a list of items in any order (free recall), people tend to begin recall with the end of the list, recalling those items best (the recency effect). Among earlier list items, the first few items are recalled more frequently than the middle items (the primacy effect).[3][4]\n", "One suggested reason for the primacy effect is that the initial items presented are most effectively stored in long-term memory because of the greater amount of processing devoted to them. (The first list item can be rehearsed by itself; the second must be rehearsed along with the first, the third along with the first and second, and so on.) The primacy effect is reduced when items are presented quickly and is enhanced when presented slowly (factors that reduce and enhance processing of each item and thus permanent storage). Longer presentation lists have been found to reduce the primacy effect.[4]\n", "One theorised reason for the recency effect is that these items are still present in working memory when recall is solicited. Items that benefit from neither (the middle items) are recalled most poorly. An additional explanation for the recency effect is related to temporal context: if tested immediately after rehearsal, the current temporal context can serve as a retrieval cue, which would predict more recent items to have a higher likelihood of recall than items that were studied in a different temporal context (earlier in the list).[5] The recency effect is reduced when an interfering task is given. Intervening tasks involve working memory, as the distractor activity, if exceeding 15 to 30 seconds in duration, can cancel out the recency effect.[6] Additionally, if recall comes immediately after the test, the recency effect is consistent regardless of the length of the studied list,[4] or presentation rate.[7]\n", "Amnesiacs with poor ability to form permanent long-term memories do not show a primacy effect, but do show a recency effect if recall comes immediately after study.[8] People with Alzheimer's disease exhibit a reduced primacy effect but do not produce a recency effect in recall.[9]\n"]},
{"name": "Part-list cueing effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Memory_inhibition#Part-set_cuing_effect", "ptags": ["In psychology, memory inhibition is the ability not to remember irrelevant information. The scientific concept of memory inhibition should not be confused with everyday uses of the word \"inhibition\". Scientifically speaking, memory inhibition is a type of cognitive inhibition, which is the stopping or overriding of a mental process, in whole or in part, with or without intention.[1]\n", "Memory inhibition is a critical component of an effective memory system.[2] While some memories are retained for a lifetime, most memories are forgotten.[3] According to evolutionary psychologists, forgetting is adaptive because it facilitates selectivity of rapid, efficient recollection.[4] For example, a person trying to remember where they parked their car would not want to remember every place they have ever parked. In order to remember something, therefore, it is essential not only to activate the relevant information, but also to inhibit irrelevant information. \n", "There are many memory phenomena that seem to involve inhibition, although there is often debate about the distinction between interference and inhibition.\n"]},
{"name": "Serial position effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Serial-position_effect", "ptags": ["Serial-position effect is the tendency of a person to recall the first and last items in a series best, and the middle items worst.[1] The term was coined by Hermann Ebbinghaus through studies he performed on himself, and refers to the finding that recall accuracy varies as a function of an item's position within a study list.[2] When asked to recall a list of items in any order (free recall), people tend to begin recall with the end of the list, recalling those items best (the recency effect). Among earlier list items, the first few items are recalled more frequently than the middle items (the primacy effect).[3][4]\n", "One suggested reason for the primacy effect is that the initial items presented are most effectively stored in long-term memory because of the greater amount of processing devoted to them. (The first list item can be rehearsed by itself; the second must be rehearsed along with the first, the third along with the first and second, and so on.) The primacy effect is reduced when items are presented quickly and is enhanced when presented slowly (factors that reduce and enhance processing of each item and thus permanent storage). Longer presentation lists have been found to reduce the primacy effect.[4]\n", "One theorised reason for the recency effect is that these items are still present in working memory when recall is solicited. Items that benefit from neither (the middle items) are recalled most poorly. An additional explanation for the recency effect is related to temporal context: if tested immediately after rehearsal, the current temporal context can serve as a retrieval cue, which would predict more recent items to have a higher likelihood of recall than items that were studied in a different temporal context (earlier in the list).[5] The recency effect is reduced when an interfering task is given. Intervening tasks involve working memory, as the distractor activity, if exceeding 15 to 30 seconds in duration, can cancel out the recency effect.[6] Additionally, if recall comes immediately after the test, the recency effect is consistent regardless of the length of the studied list,[4] or presentation rate.[7]\n", "Amnesiacs with poor ability to form permanent long-term memories do not show a primacy effect, but do show a recency effect if recall comes immediately after study.[8] People with Alzheimer's disease exhibit a reduced primacy effect but do not produce a recency effect in recall.[9]\n"]},
{"name": "Suffix effect", "link": "https:\/\/en.wikipedia.org\/wiki\/List_of_cognitive_biases#Suffix_effect", "ptags": ["Cognitive biases are systematic patterns of deviation from norm and\/or rationality in judgment. They are often studied in psychology, sociology and behavioral economics.[1]\n", "Although the reality of most of these biases is confirmed by reproducible research,[2][3] there are often controversies about how to classify these biases or how to explain them.[4] Several theoretical causes are known for some cognitive biases, which provides a classification of biases by their common generative mechanism (such as noisy information-processing[5]). Gerd Gigerenzer has criticized the framing of cognitive biases as errors in judgment, and favors interpreting them as arising from rational deviations from logical thought.[6]\n", "Explanations include information-processing rules (i.e., mental shortcuts), called heuristics, that the brain uses to produce decisions or judgments. Biases have a variety of forms and appear as cognitive (\"cold\") bias, such as mental noise,[5] or motivational (\"hot\") bias, such as when beliefs are distorted by wishful thinking. Both effects can be present at the same time.[7][8]\n", "There are also controversies over some of these biases as to whether they count as useless or irrational, or whether they result in useful attitudes or behavior. For example, when getting to know others, people tend to ask leading questions which seem biased towards confirming their assumptions about the person. However, this kind of confirmation bias has also been argued to be an example of social skill; a way to establish a connection with the other person.[9]\n", "Although this research overwhelmingly involves human subjects, some findings that demonstrate bias have been found in non-human animals as well. For example, loss aversion has been shown in monkeys and hyperbolic discounting has been observed in rats, pigeons, and monkeys.[10]\n"]}
   
        ]
       },
   
       {
        "name": "We store memories differently based on how they were experienced",
        "children": [
   
{"name": "Levels of processing effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Levels-of-processing_effect", "ptags": ["The Levels of Processing model, created by Fergus I. M. Craik and Robert S. Lockhart in 1972, describes memory recall of stimuli as a function of the depth of mental processing.  More analysis produce more elaborate and stronger memory than lower levels of processing. Depth of processing falls on a shallow to deep continuum.[citation needed] Shallow processing (e.g., processing based on phonemic and orthographic components) leads to a fragile memory trace that is susceptible to rapid decay. Conversely, deep processing (e.g., semantic processing) results in a more durable memory trace.[1]  There are three levels of processing in this model. Structural processing, or visual, is when we remember only the physical quality of the word E.g how the word is spelled and how letters look. Phonemic processing includes remembering the word by the way it sounds. E.G the word tall rhymes with fall. Lastly, we have semantic processing in which we encode the meaning of the word with another word that is similar or has similar meaning. Once the word is perceived, the brain allows for a deeper processing.  \n", "This theory contradicts the multi-store Atkinson-Shiffrin memory model which represents memory strength as being continuously variable, the assumption being that rehearsal always improves long-term memory.  They argued that rehearsal that consists simply of repeating previous analyses (maintenance rehearsal) doesn't enhance long-term memory.[2]\n", "In a study from 1975 (Craik and Tulving) participants were given a list of 60 words. Each word was presented along with three questions. The participant had to answer one of them. Those three questions were in one of three categories. One category of questions was about how the word was presented visually (\"Is the word shown in italics?\"). The second category of questions was about the phonemic qualities of the word (\"Does the word begin with the sound 'bee'?\"). The third category of questions was presented so that the reader was forced to think about the word within a certain context. (\"Can you meet one in the street [a friend]\"?) The result of this study showed that the words which contained deep processing (the latter) were remembered better.[3]\n"]},
{"name": "Absent-mindedness", "link": "https:\/\/en.wikipedia.org\/wiki\/Absent-mindedness", "ptags": ["In the field of psychology, absent-mindedness is a mental state wherein a person is forgetfully inattentive.[1] It is the opposite mental state of mindfulness.\n", "Absentmindedness is often caused by things such as boredom, sleepiness, rumination, distraction, or preoccupation with one's own internal monologue. When experiencing absent-mindedness, people exhibit signs of memory lapses and weak recollection of recent events.\n", "Absent-mindedness can usually be a result of a variety of other conditions often diagnosed by clinicians, such as attention deficit hyperactivity disorder and depression. In addition to absent-mindedness leading to an array of consequences affecting daily life, it can have more severe, long-term problems.\n"]},
{"name": "Testing effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Testing_effect", "ptags": ["The testing effect (also known as retrieval practice, active recall, practice testing, or test-enhanced learning)[1][2][3] suggests long-term memory is increased when part of the learning period is devoted to retrieving information from memory.[4] It is different from the more general practice effect, defined in the APA Dictionary of Psychology as \"any change or improvement that results from practice or repetition of task items or activities.\"[5]\n", "Cognitive psychologists are working with educators to look at how to take advantage of tests\u2014not as an assessment tool, but as a teaching tool [6] since testing prior knowledge is more beneficial for learning when compared to only reading or passively studying material (even more so when the test is more challenging for memory).[7]\n"]},
{"name": "Next-in-line effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Next-in-line_effect", "ptags": ["The next-in-line effect is the phenomenon of people being unable to recall information concerning events immediately preceding their turn to perform.\n", "The effect was first studied experimentally by Malcolm Brenner in 1973. In his experiment the participants were each in turn reading a word aloud from an index card, and after 25 words were asked to recall as many of all the read words as possible. The results of the experiment showed that words read aloud within approximately nine seconds before the subject's own turn were recalled worse than other words.[1]\n", "The reason for the next-in-line effect appears to be a deficit in encoding the perceived information preceding a performance. That is, the information is never stored to long-term memory and thus cannot be retrieved later after the performance.[2][3] One finding supporting this theory is that asking the subjects beforehand to pay more attention to events preceding their turn to perform can prevent the memory deficit and even result in overcompensation, making people remember the events before their turn better than others.[3]\n", "In addition, the appearance of the next-in-line effect does not seem to be connected to the level of fear of negative evaluation. Both people with lower and higher anxiety levels are subject to the memory deficit.[4]\n"]},
{"name": "Google effect", "link": "https:\/\/en.wikipedia.org\/wiki\/Google_effect", "ptags": ["The Google effect, also called digital amnesia,[1] is the tendency to forget information that can be found readily online by using Internet search engines. According to the first study about the Google effect, people are less likely to remember certain details they believe will be accessible online. However, the study also claims that people's ability to learn information offline remains the same.[2] This effect may also be seen as a change to what information and what level of detail is considered to be important to remember.\n"]},
{"name": "Tip of the tongue phenomenon", "link": "https:\/\/en.wikipedia.org\/wiki\/Tip_of_the_tongue", "ptags": ["Tip of the tongue (also known as TOT, or lethologica) is the phenomenon of failing to retrieve a word or term from memory, combined with partial recall and the feeling that retrieval is imminent.[1] The phenomenon's name comes from the saying, \"It's on the tip of my tongue.\"[2][3][4] The tip of the tongue phenomenon reveals that lexical access occurs in stages.[5][6]\n", "People experiencing the tip-of-the-tongue phenomenon can often recall one or more features of the target word, such as the first letter, its syllabic stress, and words similar in sound, meaning, or both sound and meaning.[3] Individuals report a feeling of being seized by the state, feeling something like mild anguish while searching for the word, and a sense of relief when the word is found.[3][7] While many aspects of the tip-of-the-tongue state remain unclear, there are two major competing explanations for its occurrence: the direct-access view and the inferential view. Emotion and the strength of the emotional ties to what is trying to be remembered can also have an impact on the TOT phenomenon. The stronger the emotional ties, the longer it takes to retrieve the item from memory.[8]\n", "TOT states should be distinguished from FOK (feeling of knowing) states. FOK, in contrast, is the feeling that one will be able to recognize\u2060\u2014from a list of items\u2060\u2014an item that is currently inaccessible. There are still currently opposing hypotheses in the psychological literature regarding the separability of the process underlying these concepts. However, there is some evidence that TOTs and FOKs draw on different parts of the brain. TOTs are associated with the anterior cingulate, right dorsolateral prefrontal cortex, and right inferior cortex while FOKs are not.[9] FOKs can be assessed through memory-monitoring testing in which a test subject is asked to \"estimate the likelihood\" of recognizing when \"prompted with a cue\" or information that they previously failed to remember.[10][11] This test aims to measure a test subject's accuracy of memory monitoring during the \"memory extraction stage\".[12][10]\n", "An occasional tip-of-the-tongue state is normal for people of all ages; however, it becomes more frequent as people age.[1] TOT can be referred as an actual medical condition, but only when it becomes frequent enough to interfere with learning or daily life. This disorder is called anomic aphasia when acquired by brain damage, usually from a head injury, stroke, or dementia.[13]\n", "The tip of the tongue phenomenon has implications for research in psycholinguistics, memory, and metacognition.[2]\n"]}
   
        ]
       }
   
      ]
     }
   
    ]
   }
   
   